{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_EfficientNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEBSCHELLI/DBpedia_Img2Ent/blob/master/Final_EfficientNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tevrDHuvXsz",
        "colab_type": "text"
      },
      "source": [
        "#Imports and Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvwBL6E4brTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from io import BytesIO\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "# Clone Google Github TPU directory and append paths to system\n",
        "if not os.path.exists('tpu'):\n",
        "  !git clone https://github.com/tensorflow/tpu/\n",
        "\n",
        "sys.path.append('/content/tpu/models/official/efficientnet')\n",
        "sys.path.append('/content/tpu/models/common')\n",
        "sys.path.append('/content/tpu/tools/datasets')\n",
        "\n",
        "# Import tf v1.15.0, enable eager execution, import tf v2 for summaries\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "print(\"Running on Tensorflow v{}\".format(tf.__version__))\n",
        "import tensorflow.compat.v2 as tf2  # used for summaries only.\n",
        "from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "from tensorflow.python.estimator import estimator\n",
        "\n",
        "\n",
        "# Import files from cloned Efficientnet Github Reo\n",
        "import imagenet_input\n",
        "import efficientnet_builder\n",
        "import utils\n",
        "import preprocessing\n",
        "\n",
        "\n",
        "# Import Google Cloud modules to access Google Cloud Storage Buckets\n",
        "from google.cloud import storage\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import google.auth\n",
        "\n",
        "# Install and import Google Cloud Storage File System\n",
        "!pip install gcsfs\n",
        "import gcsfs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQlErh0nkEwO",
        "colab_type": "text"
      },
      "source": [
        "Set Logging Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qETQZ5klunYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Log all events\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  \n",
        "logging.getLogger('tensorflow').setLevel(logging.INFO)\n",
        "logging.disable(logging.NOTSET)\n",
        "tf.get_logger().propagate = False\n",
        "\n",
        "\n",
        "# Only log fatal errors\n",
        "\"\"\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "logging.disable(logging.WARNING)\n",
        "tf.get_logger().propagate = False\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "988L0eU6kNl5",
        "colab_type": "text"
      },
      "source": [
        "Connect to TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsTj7camKEBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assert that notebook is connected to a TPU runtime\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "# Connect with TPU with Google Credentials\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  print(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FfO2PMoVFKT",
        "colab_type": "text"
      },
      "source": [
        "# Training Fully Connected Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89vMDnZmfhN_",
        "colab_type": "text"
      },
      "source": [
        "###Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lCO8YysdJNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"efficientnet-b0\" #@param [\"efficientnet-b0\", \"efficientnet-b5\"] {type:\"string\"}\n",
        "data_set = \"dataset_tf_10000-100\" #@param [\"dataset-tf-XXL\",\"dataset-validation\",\"dataset-labeled\", \"dataset_tfrecords\", \"dataset_tf_10000-100\", \"cifar100\", \"dataset-tf-balanced\", \"dataset-filtered\"] {type:\"string\"}\n",
        "\n",
        "# run_name as a unique identifier to specify training run\n",
        "run_name = \"FT-28-01-20-8\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhvCX495ff0P",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Model and dataset parameters depending on the selection above\n",
        "model_params = {\n",
        "    \"efficientnet-b0\":{\"iterations_per_loop\":100, \"train_batch_size\":1024},\n",
        "    \"efficientnet-b5\":{\"iterations_per_loop\":128, \"train_batch_size\":1024}\n",
        "                }\n",
        "\n",
        "data_set_params = {\n",
        "    \"dataset_tfrecords\":{\"num_train_images\":33021, \"num_eval_images\":8257, \"num_label_classes\":96,\"data_dir\":\"gs://ise-bucket/efficientnet/dataset_tfrecords\"},\n",
        "    \"dataset_tf_10000-100\":{\"num_train_images\":169733, \"num_eval_images\":42434, \"num_label_classes\":100,\"data_dir\":'gs://ise-bucket/efficientnet/dataset-tf-20-01-13_1-img_10000-ent_100-class'},\n",
        "    \"cifar100\":{\"num_train_images\":50000, \"num_eval_images\":10000, \"num_label_classes\":100,\"data_dir\":\"gs://ise-bucket/cifar100\"},\n",
        "    \"dataset-tf-balanced\":{\"num_train_images\":159460, \"num_eval_images\":39866, \"num_label_classes\":100,\"data_dir\":'gs://ise-bucket/efficientnet/dataset-tf-20-01-15_1-img_2000-ent_100-class'},\n",
        "    \"dataset-filtered\":{\"num_train_images\":156271, \"num_eval_images\":38173, \"num_label_classes\":100,\"data_dir\":'gs://ise-bucket/efficientnet/dataset-tf-20-01-29_1-img_2000-ent_100-class'},\n",
        "    \"dataset-labeled\":{\"num_train_images\":0, \"num_eval_images\":74228, \"num_label_classes\":100,\"data_dir\":'gs://ise-bucket/efficientnet/dataset-tf-20-01-31_1-img_2000-ent_100-class_specific'},\n",
        "    \"dataset-validation\":{\"num_train_images\":0, \"num_eval_images\":29162, \"num_label_classes\":100,\"data_dir\":'gs://ise-bucket/efficientnet/dataset-tf-20-02-01_1-img_2000-ent_100-class_specific'},\n",
        "    \"dataset-tf-XXL\":{\"num_train_images\":683484, \"num_eval_images\":170871, \"num_label_classes\":100,\"data_dir\":'gs://ise-bucket/efficientnet/dataset-tf-20-01-22_1-img_10000-ent_100-class'}\n",
        "\n",
        "}\n",
        "\n",
        "model_params = model_params[model_name]\n",
        "data_set_params = data_set_params[data_set]\n",
        "\n",
        "\n",
        "# Define model directory, where checkpoints are stored\n",
        "model_dir = 'gs://ise-bucket/efficientnet/'+model_name+'/'+run_name\n",
        "\n",
        "\n",
        "# Define, whether to run with pretrained weight, warm start or from scratch \n",
        "# Run with pretrained weights\n",
        "init_checkpoint_dir = 'gs://ise-bucket/efficientnet/'+model_name+'/'\n",
        "\n",
        "# Run with warm start fc head\n",
        "#init_checkpoint_dir = 'gs://ise-bucket/efficientnet/'+model_name+'/FT-27-01-20-8/'\n",
        "\n",
        "# Run on scratch\n",
        "#init_checkpoint_dir = None\n",
        "\n",
        "\n",
        "# Automatically set parameters\n",
        "data_dir = data_set_params[\"data_dir\"]\n",
        "iterations_per_loop = model_params[\"iterations_per_loop\"]\n",
        "num_train_images = data_set_params[\"num_train_images\"]\n",
        "num_eval_images =  data_set_params[\"num_eval_images\"]\n",
        "train_batch_size = model_params[\"train_batch_size\"]\n",
        "eval_batch_size = 1024\n",
        "predict_batch_size = 64\n",
        "train_steps = int(num_train_images / train_batch_size * num_epochs)\n",
        "eval_steps = int(num_eval_images / eval_batch_size)\n",
        "num_label_classes = data_set_params[\"num_label_classes\"]\n",
        "input_image_size = efficientnet_builder.efficientnet_params(model_name)[2] \n",
        "\n",
        "use_tpu = True\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "skip_host_call = False\n",
        "num_parallel_calls = 8\n",
        "transpose_input=True\n",
        "use_cache = True\n",
        "include_background_label = False\n",
        "\n",
        "\n",
        "# Define further parameters\n",
        "num_epochs = 200\n",
        "log_step_count_steps = 64 #produce log every x steps\n",
        "mixup_alpha = 0.0 \n",
        "augment_name = None #whether to use randaugment - '\"randaugment\"\n",
        "randaug_num_layers = 0 #value between 1-3 for randaugment\n",
        "randaug_magnitude = 0 #value between 5-30 for randaugment\n",
        "moving_average_decay = 0.9999\n",
        "label_smoothing = 0.1\n",
        "weight_decay = 1e-5\n",
        "base_learning_rate = 0.016\n",
        "lr_decay = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuADkowqfA1E",
        "colab_type": "text"
      },
      "source": [
        "###Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl7ZOErYfDda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ema_vars():\n",
        "  \"\"\"Get all exponential moving average (ema) variables.\"\"\"\n",
        "  ema_vars = tf.trainable_variables() + tf.get_collection('moving_vars')\n",
        "  for v in tf.global_variables():\n",
        "    # We maintain mva for batch norm moving mean and variance as well.\n",
        "    if 'moving_mean' in v.name or 'moving_variance' in v.name:\n",
        "      ema_vars.append(v)\n",
        "  return list(set(ema_vars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4RazARwYxlq",
        "colab_type": "text"
      },
      "source": [
        "###InputFn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJejC6kkcagf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(is_training):\n",
        "  def input_fn(params):\n",
        "      \"\"\"Input function which provides a single batch for train or eval.\n",
        "      Args:\n",
        "        params: `dict` of parameters passed from the `TPUEstimator`.\n",
        "            `params['batch_size']` is always provided and should be used as the\n",
        "            effective batch size.\n",
        "      Returns:\n",
        "        A `tf.data.Dataset` object.\n",
        "      \"\"\"\n",
        "\n",
        "      print(params)\n",
        "\n",
        "      # Retrieves the batch size for the current shard. The # of shards is\n",
        "      # computed according to the input pipeline deployment. See\n",
        "      # tf.estimator.tpu.RunConfig for details.\n",
        "      batch_size = params['batch_size'] #batch_size = train_batch_size / num_cores\n",
        "\n",
        "      num_cores = 8\n",
        "      imagenet_train = imagenet_input.ImageNetInput(\n",
        "        is_training=is_training,\n",
        "        data_dir=data_dir,\n",
        "        transpose_input=transpose_input,\n",
        "        cache=use_cache and is_training,\n",
        "        image_size=input_image_size,\n",
        "        num_parallel_calls=num_parallel_calls,\n",
        "        use_bfloat16=False,\n",
        "        num_label_classes=num_label_classes,\n",
        "        include_background_label=include_background_label,\n",
        "        augment_name=augment_name,\n",
        "        mixup_alpha=mixup_alpha,\n",
        "        randaug_num_layers=randaug_num_layers,\n",
        "        randaug_magnitude=randaug_magnitude)\n",
        "\n",
        "      if 'context' in params:\n",
        "        current_host = params['context'].current_input_fn_deployment()[1]\n",
        "        num_hosts = params['context'].num_hosts\n",
        "      else:\n",
        "        current_host = 0\n",
        "        num_hosts = 1\n",
        "\n",
        "      dataset = imagenet_train.make_source_dataset(current_host, num_hosts)\n",
        "      \n",
        "      # Use the fused map-and-batch operation.\n",
        "      #\n",
        "      # For XLA, we must used fixed shapes. Because we repeat the source training\n",
        "      # dataset indefinitely, we can use `_remainder=True` to get fixed-size\n",
        "      # batches without dropping any training examples.\n",
        "      #\n",
        "      # When evaluating, `drop_remainder=True` prevents accidentally evaluating\n",
        "      # the same image twice by dropping the final batch if it is less than a full\n",
        "      # batch size. As long as this validation is done with consistent batch size,\n",
        "      # exactly the same images will be used.\n",
        "\n",
        "      dataset = dataset.apply(\n",
        "          tf.data.experimental.map_and_batch(\n",
        "              imagenet_train.dataset_parser, batch_size=batch_size,\n",
        "              num_parallel_batches=num_cores, drop_remainder=True))\n",
        "      \n",
        "      # Apply Mixup\n",
        "      if imagenet_train.is_training and imagenet_train.mixup_alpha > 0.0:\n",
        "        dataset = dataset.map(\n",
        "            functools.partial(imagenet_train.mixup, batch_size, imagenet_train.mixup_alpha),\n",
        "            num_parallel_calls=num_cores)\n",
        "\n",
        "      \n",
        "      # Transpose for performance on TPU\n",
        "      if imagenet_train.transpose_input:\n",
        "        dataset = dataset.map(\n",
        "            lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels),\n",
        "            num_parallel_calls=num_cores)\n",
        "\n",
        "      # Assign static batch size dimension\n",
        "      dataset = dataset.map(functools.partial(imagenet_train.set_shapes, batch_size))\n",
        "      \n",
        "\n",
        "      # Prefetch overlaps in-feed with training\n",
        "      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "      \n",
        "      return dataset\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU5KhZ8gmc3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder_unshuffled(is_training, only_filenames=False):\n",
        "  def input_fn(params):\n",
        "      \"\"\"Input function which provides data withouth shuffling for evaluation.\n",
        "      Args:\n",
        "        params: `dict` of parameters passed from the `TPUEstimator`.\n",
        "            `params['batch_size']` is always provided and should be used as the\n",
        "            effective batch size.\n",
        "      Returns:\n",
        "        A `tf.data.Dataset` object.\n",
        "      \"\"\"\n",
        "\n",
        "      num_cores = 8\n",
        "    \n",
        "      if 'context' in params:\n",
        "        current_host = params['context'].current_input_fn_deployment()[1]\n",
        "        num_hosts = params['context'].num_hosts\n",
        "      else:\n",
        "        current_host = 0\n",
        "        num_hosts = 1\n",
        "\n",
        "      if \"batch_size\" in params:\n",
        "        batch_size = params['batch_size']\n",
        "          \n",
        "      file_pattern = os.path.join(\n",
        "          data_dir, 'train-*' if is_training else 'validation-*')\n",
        "\n",
        "      dataset = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n",
        "      dataset = dataset.shard(num_hosts, current_host)\n",
        "\n",
        "      def fetch_dataset(filename):\n",
        "        buffer_size = 8 * 1024 * 1024  # 8 MiB per file\n",
        "        dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n",
        "        return dataset\n",
        "\n",
        "      # Read the data from disk in parallel\n",
        "      dataset = dataset.apply(\n",
        "          tf.data.experimental.parallel_interleave(\n",
        "              fetch_dataset, cycle_length=num_parallel_calls, sloppy=False))\n",
        "\n",
        "      \n",
        "      def custom_dataset_parser(value):\n",
        "        keys_to_features = {\n",
        "            'image/encoded': tf.FixedLenFeature((), tf.string, ''),\n",
        "            'image/class/label': tf.FixedLenFeature([], tf.int64, -1),\n",
        "            'image/filename': tf.FixedLenFeature([], tf.string, ''),\n",
        "            'image/class/text': tf.FixedLenFeature([], tf.string, '')\n",
        "        }\n",
        "\n",
        "        parsed = tf.parse_single_example(value, keys_to_features)\n",
        "        \n",
        "        image_bytes = tf.reshape(parsed['image/encoded'], shape=[])\n",
        "\n",
        "        \n",
        "        image = preprocessing.preprocess_image(\n",
        "            image_bytes=image_bytes,\n",
        "            is_training=is_training,\n",
        "            image_size=input_image_size,\n",
        "            use_bfloat16=False,\n",
        "            augment_name=augment_name,\n",
        "            randaug_num_layers=randaug_num_layers,\n",
        "            randaug_magnitude=randaug_magnitude)\n",
        "        \n",
        "              \n",
        "        label = tf.cast(tf.reshape(parsed['image/class/label'], shape=[]), dtype=tf.int32)\n",
        "        onehot_label = tf.one_hot(label, num_label_classes)\n",
        "\n",
        "        label_text = parsed['image/class/text']\n",
        "        file_name = parsed['image/filename']\n",
        "\n",
        "        if only_filenames:\n",
        "          return label_text, file_name\n",
        "        return image, onehot_label\n",
        "\n",
        "      dataset = dataset.apply(\n",
        "          tf.data.experimental.map_and_batch(\n",
        "              custom_dataset_parser, batch_size=batch_size,\n",
        "              num_parallel_batches=num_cores, drop_remainder=False))\n",
        "\n",
        "      # Prefetch overlaps in-feed with training\n",
        "      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "      \n",
        "      return dataset\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP6EevGXY1Vr",
        "colab_type": "text"
      },
      "source": [
        "###ModelFn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmYrkTdskwTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "  \"\"\"The model_fn to be used with TPUEstimator.\n",
        "  Args:\n",
        "    features: `Tensor` of batched images.\n",
        "    labels: `Tensor` of one hot labels for the data samples\n",
        "    mode: one of `tf.estimator.ModeKeys.{TRAIN,EVAL,PREDICT}`\n",
        "    params: `dict` of parameters passed to the model from the TPUEstimator,\n",
        "        `params['batch_size']` is always provided and should be used as the\n",
        "        effective batch size.\n",
        "  Returns:\n",
        "    A `TPUEstimatorSpec` for the model\n",
        "  \"\"\"\n",
        "  if isinstance(features, dict):\n",
        "    print(\"is dict\")\n",
        "    features = features['feature']\n",
        "\n",
        "  stats_shape = [1, 1, 3]\n",
        "\n",
        "  if transpose_input and mode != tf.estimator.ModeKeys.PREDICT:\n",
        "    features = tf.transpose(features, [3, 0, 1, 2])  # HWCN to NHWC\n",
        "\n",
        "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "  has_moving_average_decay = (moving_average_decay > 0)\n",
        "\n",
        "  # This is essential, if using a keras-derived model.\n",
        "  tf.keras.backend.set_learning_phase(is_training)\n",
        "  tf.logging.info('Using open-source implementation.')\n",
        "  override_params = {}\n",
        "\n",
        "  def normalize_features(features, mean_rgb, stddev_rgb):\n",
        "    \"\"\"Normalize the image given the means and stddevs.\"\"\"\n",
        "    features -= tf.constant(mean_rgb, shape=stats_shape, dtype=features.dtype)\n",
        "    features /= tf.constant(stddev_rgb, shape=stats_shape, dtype=features.dtype)\n",
        "    return features\n",
        "\n",
        "  def build_model():\n",
        "    \"\"\"Build model using the model_name given through the command line.\"\"\"\n",
        "    model_builder = efficientnet_builder\n",
        "    normalized_features = normalize_features(features, model_builder.MEAN_RGB,\n",
        "                                             model_builder.STDDEV_RGB)\n",
        "    \n",
        "    output_layer, endpoints = model_builder.build_model(\n",
        "        normalized_features,\n",
        "        model_name=model_name,\n",
        "        fine_tuning = True,\n",
        "        pooled_features_only = False,\n",
        "        features_only = False,\n",
        "        training=is_training,\n",
        "        override_params=override_params,\n",
        "        model_dir=model_dir)\n",
        "\n",
        "      \n",
        "    # Connect a new Dense Layer to the global pooling features of the pretrained CNN\n",
        "    def dense_kernel_initializer(shape, dtype=None, partition_info=None):\n",
        "      del partition_info\n",
        "      init_range = 1.0 / np.sqrt(shape[1])\n",
        "      return tf.random_uniform(shape, -init_range, init_range, dtype=dtype)\n",
        "\n",
        "    _fc = tf.layers.Dense(\n",
        "            num_label_classes,\n",
        "            kernel_initializer=dense_kernel_initializer,name = \"ldense\")\n",
        "      \n",
        "    logits = _fc(endpoints['global_pool']) #connect to global pooling layer of pretrained CNN (other endpoints possible)\n",
        "    \n",
        "\n",
        "    return logits\n",
        "\n",
        "  logits = build_model()\n",
        "\n",
        "  train_op = None\n",
        "  host_call = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # PREDICTION MODE\n",
        "  # ******************************************************************************\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        'label': tf.argmax(labels, axis=1),\n",
        "        'prediction': tf.argmax(logits, axis=1),\n",
        "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
        "    }\n",
        "    return tf.estimator.tpu.TPUEstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions=predictions,\n",
        "        export_outputs={\n",
        "            'classify': tf.estimator.export.PredictOutput(predictions)\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # CONTINUE WITH TRAINING OR EVAL MODE\n",
        "  # ******************************************************************************\n",
        "  batch_size = params['batch_size']\n",
        "\n",
        "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
        "  cross_entropy = tf.losses.softmax_cross_entropy(\n",
        "      logits=logits,\n",
        "      onehot_labels=labels,\n",
        "      label_smoothing=label_smoothing)\n",
        "\n",
        "  # Add weight decay to the loss for non-batch-normalization variables.\n",
        "  loss = cross_entropy + weight_decay * tf.add_n(\n",
        "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n",
        "       if 'batch_normalization' not in v.name])\n",
        "\n",
        "  global_step = tf.train.get_global_step()\n",
        "  tf.logging.info(\"globalstep:{}\".format(global_step))\n",
        "\n",
        "  restore_vars_dict = None\n",
        "  if has_moving_average_decay:\n",
        "    ema = tf.train.ExponentialMovingAverage(decay=moving_average_decay, num_updates=global_step)\n",
        "    ema_vars = get_ema_vars()\n",
        "    restore_vars_dict = ema.variables_to_restore(ema_vars)\n",
        "  \n",
        "  # TRAINING MODE\n",
        "  # ******************************************************************************\n",
        "  if is_training:\n",
        "    # Compute the current epoch and associated learning rate from global_step.\n",
        "    current_epoch = (tf.cast(global_step, tf.float32) / params['steps_per_epoch'])\n",
        "    tf.logging.info('current epoch = {}'.format(tf.reshape(current_epoch, [1])))\n",
        "    \n",
        "    scaled_lr = base_learning_rate * (train_batch_size / 256.0)\n",
        "    tf.logging.info('base_learning_rate = %f', base_learning_rate)\n",
        "    learning_rate = utils.build_learning_rate(scaled_lr, global_step,params['steps_per_epoch'])\n",
        "    \n",
        "    # Override scaled learning rate with base_learning_rate (comment if scaled lr decay be used)\n",
        "    if not lr_decay:\n",
        "      learning_rate = scaled_lr\n",
        "    \n",
        "    optimizer = utils.build_optimizer(learning_rate)\n",
        "    print(\"learning rate:\",learning_rate)\n",
        "\n",
        "    optimizer = tf.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "    # Batch normalization requires UPDATE_OPS to be added as a dependency to\n",
        "    # the train operation.\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "    \n",
        "    vars_to_update = [var for var in tf.trainable_variables() if 'ldense' in var.name]\n",
        "    print(vars_to_update)\n",
        "    \n",
        "    with tf.control_dependencies(update_ops):\n",
        "      train_op = optimizer.minimize(loss, global_step, var_list=vars_to_update)\n",
        "\n",
        "\n",
        "    if has_moving_average_decay:\n",
        "      with tf.control_dependencies([train_op]):\n",
        "        train_op = ema.apply(ema_vars)\n",
        "\n",
        "\n",
        "    # Host Call Function to write scalar summaries for Tensorboard\n",
        "    def host_call_fn(gs, lr, ce,acc):\n",
        "      print(\"host call\")\n",
        "      gs = gs[0]\n",
        "\n",
        "      with tf2.summary.create_file_writer(model_dir, max_queue=iterations_per_loop).as_default():\n",
        "        with tf2.summary.record_if(True):\n",
        "          tf2.summary.scalar('learning_rate', lr[0], step=gs)\n",
        "          tf2.summary.scalar('current_epoch', ce[0], step=gs)\n",
        "          tf2.summary.scalar('accuracy', acc[0], step=gs)\n",
        "        return tf.summary.all_v2_summary_ops()\n",
        "\n",
        "    gs_t = tf.reshape(global_step, [1])\n",
        "    lr_t = tf.reshape(learning_rate, [1])\n",
        "    ce_t = tf.reshape(current_epoch, [1])\n",
        "\n",
        "    labels = tf.argmax(labels, axis=1)\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32)) # own accuracy metric\n",
        "    accuracy_t = tf.reshape(accuracy, [-1])\n",
        "\n",
        "    host_call = (host_call_fn, [gs_t, lr_t, ce_t,accuracy_t])\n",
        "\n",
        "\n",
        "    if init_checkpoint_dir:\n",
        "\n",
        "      num_params = np.sum([np.prod(v.shape) for v in tf.trainable_variables()])\n",
        "      tf.logging.info('number of trainable parameters: %d', num_params)\n",
        "\n",
        "      # TPU Scaffold Function to load variables from initial pretrained checkpoint \n",
        "      def tpu_scaffold():\n",
        "        print(\"init_checkpoint_dir: {}\".format(init_checkpoint_dir))\n",
        "\n",
        "        # Remove variables that are not stored in pretrained checkpoint\n",
        "        restore_vars_dict.pop(\"global_step\")\n",
        "        #restore_vars_dict.pop(\"output_bias/ExponentialMovingAverage\")\n",
        "        #restore_vars_dict.pop(\"output_weights/ExponentialMovingAverage\")\n",
        "        restore_vars_dict.pop(\"ldense/bias/ExponentialMovingAverage\")\n",
        "        restore_vars_dict.pop(\"ldense/kernel/ExponentialMovingAverage\")\n",
        "        tf.train.init_from_checkpoint(init_checkpoint_dir, restore_vars_dict)\n",
        "        print(\"init checkpoint finished\")\n",
        "        return tf.train.Scaffold()\n",
        "\n",
        "      scaffold_fn = tpu_scaffold\n",
        "\n",
        "      return tf.estimator.tpu.TPUEstimatorSpec(\n",
        "        mode=mode,\n",
        "        loss=loss,\n",
        "        train_op=train_op,\n",
        "        host_call=host_call,\n",
        "        scaffold_fn=scaffold_fn\n",
        "        )\n",
        "      \n",
        "    return tf.estimator.tpu.TPUEstimatorSpec(\n",
        "      mode=mode,\n",
        "      loss=loss,\n",
        "      train_op=train_op,\n",
        "      host_call=host_call\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # EVALUATION MODE\n",
        "  # ******************************************************************************\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "    # Function to compute confusion matrix with labels and predictions\n",
        "    def eval_confusion_matrix(labels, predictions):\n",
        "        with tf.variable_scope(\"eval_confusion_matrix\"):\n",
        "          con_matrix = tf.confusion_matrix(labels=labels, predictions=predictions, num_classes=num_label_classes)\n",
        "\n",
        "          con_matrix_sum = tf.Variable(lambda: tf.zeros(shape=(num_label_classes,num_label_classes), dtype=tf.int32),\n",
        "                                              trainable=False,\n",
        "                                              name=\"confusion_matrix_result\",\n",
        "                                              collections=[tf.GraphKeys.LOCAL_VARIABLES])\n",
        "\n",
        "\n",
        "          update_op = tf.assign_add(con_matrix_sum, con_matrix)\n",
        "\n",
        "          return tf.convert_to_tensor(con_matrix_sum), update_op\n",
        "\n",
        "    # Metric Function to return and write evaluation metrics \n",
        "    def metric_fn(labels, logits):\n",
        "      \"\"\"Evaluation metric function. Evaluates accuracy.\n",
        "      This function is executed on the CPU and should not directly reference\n",
        "      any Tensors in the rest of the `model_fn`. To pass Tensors from the model\n",
        "      to the `metric_fn`, provide as part of the `eval_metrics`. See\n",
        "      https://www.tensorflow.org/api_docs/python/tf/estimator/tpu/TPUEstimatorSpec\n",
        "      for more information.\n",
        "      Arguments should match the list of `Tensor` objects passed as the second\n",
        "      element in the tuple passed to `eval_metrics`.\n",
        "      Args:\n",
        "        labels: `Tensor` with shape `[batch, num_classes]`.\n",
        "        logits: `Tensor` with shape `[batch, num_classes]`.\n",
        "      Returns:\n",
        "        A dict of the metrics to return from evaluation.\n",
        "      \"\"\"\n",
        "      labels = tf.argmax(labels, axis=1)\n",
        "      predictions = tf.argmax(logits, axis=1)\n",
        "\n",
        "      top_1_accuracy = tf.metrics.accuracy(labels, predictions)\n",
        "      in_top_5 = tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32)\n",
        "      top_5_accuracy = tf.metrics.mean(in_top_5)\n",
        "      con_mat= eval_confusion_matrix(labels, predictions)\n",
        "\n",
        "      return {\n",
        "          'top_1_accuracy': top_1_accuracy,\n",
        "          'top_5_accuracy': top_5_accuracy,\n",
        "          'con_mat':con_mat\n",
        "      }\n",
        "\n",
        "    eval_metrics = (metric_fn, [labels, logits])\n",
        "\n",
        "    return tf.estimator.tpu.TPUEstimatorSpec(\n",
        "      mode=mode,\n",
        "      loss=loss,\n",
        "      eval_metrics=eval_metrics,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3egK46JY9h1",
        "colab_type": "text"
      },
      "source": [
        "###Run Config and Estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4FmkSDHeAJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up Run Configuration and Estimator\n",
        "params = dict(\n",
        "      steps_per_epoch=num_train_images / train_batch_size,\n",
        "      use_bfloat16=False);\n",
        "\n",
        "save_checkpoints_steps = 2*params[\"steps_per_epoch\"];\n",
        "\n",
        "config = tf.estimator.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=model_dir, #model dir used for \n",
        "      save_checkpoints_steps=save_checkpoints_steps,\n",
        "      keep_checkpoint_max = 100,\n",
        "      log_step_count_steps=log_step_count_steps,\n",
        "      session_config=tf.ConfigProto(\n",
        "          graph_options=tf.GraphOptions(\n",
        "              rewrite_options=rewriter_config_pb2.RewriterConfig(\n",
        "                  disable_meta_optimizer=True))),\n",
        "      tpu_config=tf.estimator.tpu.TPUConfig(\n",
        "          iterations_per_loop=iterations_per_loop,\n",
        "          per_host_input_for_training=tf.estimator.tpu.InputPipelineConfig\n",
        "          .PER_HOST_V2));\n",
        "\n",
        "est = tf.estimator.tpu.TPUEstimator(\n",
        "      use_tpu=use_tpu,\n",
        "      model_fn=model_fn,\n",
        "      config=config,\n",
        "      train_batch_size=train_batch_size,\n",
        "      eval_batch_size=eval_batch_size,\n",
        "      predict_batch_size = predict_batch_size,\n",
        "      params=params);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFy5SCpLinQM",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL0rhEmfipG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(epochs):\n",
        "  start_timestamp = datetime.datetime.now() # This time will include compilation time\n",
        "  tf.logging.info(\"*\"*100)\n",
        "  tf.logging.info(\"Start training for {} epochs at {}\".format(epochs, start_timestamp))\n",
        "  current_step = estimator._load_global_step_from_checkpoint_dir(model_dir)\n",
        "  steps_this_run = np.min([epochs*params['steps_per_epoch'],train_steps-current_step]) # runs for epochs minimum per run and maximum to train_steps\n",
        "  tf.logging.info(\n",
        "      'Training for %d steps (%.2f epochs in total). Current'\n",
        "      ' step %d.', steps_this_run,\n",
        "      steps_this_run / params['steps_per_epoch'], current_step)\n",
        "  tf.contrib.summary.always_record_summaries()\n",
        "  est.train(\n",
        "      input_fn=input_fn_builder(is_training=True),\n",
        "      max_steps=int(current_step+steps_this_run))\n",
        "\n",
        "  end_timestamp = datetime.datetime.now() # This time will include compilation time\n",
        "  tf.logging.info(\"Finished training at {}\".format(end_timestamp))\n",
        "  tf.logging.info(\"*\"*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEv_4X9-Aqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t2dfxc77Y2A",
        "colab_type": "text"
      },
      "source": [
        "###Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Ujysj8XNc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(checkpoint_path):\n",
        "  start_timestamp = datetime.datetime.now() # This time will include compilation time\n",
        "  tf.logging.info(\"*\"*100)\n",
        "  current_step = estimator._load_global_step_from_checkpoint_dir(model_dir)\n",
        "  epoch = current_step / params['steps_per_epoch']\n",
        "  tf.logging.info(\"Start evaluation epoch {} at {}\".format(epoch,start_timestamp))\n",
        "  predictions = est.evaluate(input_fn = input_fn_builder(is_training=False), steps = eval_steps, checkpoint_path=checkpoint_path)\n",
        "  end_timestamp = datetime.datetime.now() # This time will include compilation time\n",
        "  tf.logging.info(\"Finished evaluation at {}\".format(end_timestamp))\n",
        "  tf.logging.info(\"*\"*100)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XBPoaZ6xrR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoints_evaluated = 0\n",
        "import gcsfs\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "fs.invalidate_cache(model_dir)\n",
        "\n",
        "df_eval = pd.read_csv(model_dir+\"/checkpoint\",header=None)\n",
        "ckpt_list = [ckpt.split(\": \")[1].replace('\"','') for ckpt in df_eval[0].values[1:]]\n",
        "for ckpt in ckpt_list[74:75]:\n",
        "    checkpoint_path = model_dir+\"/\"+ckpt\n",
        "    print(checkpoint_path)\n",
        "    eval_out = evaluation(checkpoint_path)\n",
        "    #checkpoints_evaluated += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbN1_5Sk6GaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = eval_out[\"con_mat\"]\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxDO7UTu-pqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TP = np.diag(cm)\n",
        "TP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI8HoDOn-teS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FP = np.sum(cm, axis=0) - TP\n",
        "FP\n",
        "\n",
        "FN = np.sum(cm, axis=1) - TP\n",
        "FN\n",
        "\n",
        "TN = []\n",
        "sum_of_rows = np.sum(cm, axis=1)\n",
        "\n",
        "sum_labels = {}\n",
        "for i in range(num_label_classes):\n",
        "  sum_labels[labels_with_predictions_index[i]] = sum_of_rows[i]\n",
        "\n",
        "#print(sum_labels)\n",
        "\n",
        "weights = np.divide(sum_of_rows, np.sum(cm))\n",
        "\n",
        "for i in range(num_label_classes):\n",
        "    temp = np.delete(cm, i, 0)    # delete ith row\n",
        "    temp = np.delete(temp, i, 1)  # delete ith column\n",
        "    TN.append(sum(sum(temp)))\n",
        "\n",
        "TN\n",
        "precision = TP/(TP+FP)\n",
        "recall = TP/(TP+FN)\n",
        "\n",
        "precision_label = []\n",
        "for i in range(num_label_classes):\n",
        "  precision_label.append((precision[i],labels_with_predictions[i]))\n",
        "\n",
        "precision_label = sorted(precision_label, reverse=False, key=lambda x: x[0])\n",
        "print(\"precision:\", precision_label)\n",
        "\n",
        "recall_label = []\n",
        "for i in range(num_label_classes):\n",
        "  recall_label.append((recall[i],labels_with_predictions[i]))\n",
        "\n",
        "recall_label = sorted(recall_label, reverse=False, key=lambda x: x[0])\n",
        "print(\"recall:\",recall_label)\n",
        "\n",
        "\n",
        "recalls = []\n",
        "for label_idx in labels_per_level_index[1]:\n",
        "  if label_idx in labels_map_index:\n",
        "    recalls.append(recall[labels_map_index[label_idx]-1])\n",
        "    #print(all_labels[label_idx])\n",
        "    #print(recall[labels_map_index[label_idx]-1])\n",
        "\n",
        "print(\"level_1_avg_recall\",np.mean(recalls))\n",
        "\n",
        "recalls = []\n",
        "for label_idx in labels_per_level_index[2]:\n",
        "  if label_idx in labels_map_index:\n",
        "    recalls.append(recall[labels_map_index[label_idx]-1])\n",
        "\n",
        "print(\"level_2_avg_recall\",np.mean(recalls))\n",
        "\n",
        "recalls = []\n",
        "for label_idx in labels_per_level_index[3]:\n",
        "  if label_idx in labels_map_index:\n",
        "    recalls.append(recall[labels_map_index[label_idx]-1])\n",
        "\n",
        "print(\"level_3_avg_recall\",np.mean(recalls))\n",
        "\n",
        "recalls = []\n",
        "for label_idx in labels_per_level_index[4]:\n",
        "  if label_idx in labels_map_index:\n",
        "    recalls.append(recall[labels_map_index[label_idx]-1])\n",
        "    #print(all_labels[label_idx])\n",
        "\n",
        "\n",
        "print(\"level_4_avg_recall\",np.mean(recalls))\n",
        "\n",
        "print(\"----------------------\")\n",
        "\n",
        "\n",
        "precs = []\n",
        "for label_idx in labels_per_level_index[1]:\n",
        "  if label_idx in labels_map_index:\n",
        "    precs.append(precision[labels_map_index[label_idx]-1])\n",
        "    #print(all_labels[label_idx])\n",
        "    #print(precision[labels_map_index[label_idx]-1])\n",
        "\n",
        "print(\"level_1_avg_precision\",np.mean(precs))\n",
        "\n",
        "precs = []\n",
        "for label_idx in labels_per_level_index[2]:\n",
        "  if label_idx in labels_map_index:\n",
        "    precs.append(precision[labels_map_index[label_idx]-1])\n",
        "\n",
        "print(\"level_2_avg_precision\",np.mean(precs))\n",
        "\n",
        "precs = []\n",
        "for label_idx in labels_per_level_index[3]:\n",
        "  if label_idx in labels_map_index:\n",
        "    precs.append(precision[labels_map_index[label_idx]-1])\n",
        "\n",
        "print(\"level_3_avg_precision\",np.mean(precs))\n",
        "\n",
        "precs = []\n",
        "for label_idx in labels_per_level_index[4]:\n",
        "  if label_idx in labels_map_index:\n",
        "    precs.append(precision[labels_map_index[label_idx]-1])\n",
        "\n",
        "print(\"level_4_avg_precision\",np.mean(precs))\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n------------------------\\n\")\n",
        "print(\"metrics depending on number of kids per class\")\n",
        "\n",
        "num_of_kids = {0:[],1:[],2:[],3:[],4:[],5:[],7:[],8:[],10:[],11:[],21:[],26:[]}\n",
        "for label_idx in all_labels_index:\n",
        "  kids = 0\n",
        "  if label_idx in labels_map_index:\n",
        "    for h in hierarchy_dict_index.values():\n",
        "      if label_idx in h:\n",
        "        if h[-1] in labels_map_index:\n",
        "          kids += 1\n",
        "    if kids-1 in num_of_kids:\n",
        "      num_of_kids[kids-1].append(label_idx)\n",
        "    else:\n",
        "      print(all_labels[label_idx], kids)\n",
        "\n",
        "print(\"\\n\\nNumber of Images averaged per number of children\")\n",
        "for num_kids, labels in num_of_kids.items():\n",
        "  print(\"Number_of_kids:\",num_kids)\n",
        "  print(\"Number of classes:\",len(labels))\n",
        "  num_images = 0\n",
        "  for label_idx in labels:\n",
        "    if label_idx in labels_map_index:\n",
        "      num_images += sum_labels[label_idx]\n",
        "   \n",
        "  print(\"number of images:\",num_images/len(labels))\n",
        "  print(\"----------\")\n",
        "  print()\n",
        "\n",
        "    \n",
        "\n",
        "print(\"\\n\\nRecall Score averaged per number of children\")\n",
        "num_classes = 0\n",
        "mean_so_far = 0\n",
        "for num_kids, labels in num_of_kids.items():\n",
        "  print(\"Number_of_kids:\",num_kids)\n",
        "  print(\"Number of classes:\",len(labels))\n",
        "  recalls = []\n",
        "  recalls_ordered = []\n",
        "  num_classes_so_far = num_classes\n",
        "  num_classes += len(labels)\n",
        "  for label_idx in labels:\n",
        "    if label_idx in labels_map_index:\n",
        "      recalls.append(recall[labels_map_index[label_idx]-1])\n",
        "      recalls_ordered.append((recall[labels_map_index[label_idx]-1],all_labels[label_idx]))\n",
        " \n",
        "  mean_so_far = (mean_so_far*num_classes_so_far+np.mean(recalls)*len(labels))/num_classes\n",
        "  recalls_ordered = sorted(recalls_ordered, reverse=False, key=lambda x: x[0])\n",
        "  maix = min(5,len(recalls_ordered))\n",
        "  print(recalls_ordered[:maix])\n",
        "  \n",
        "  print(\"recall:\",np.mean(recalls))\n",
        "  print(\"Avg recall until here:\",(mean_so_far*num_classes_so_far+np.mean(recalls)*len(labels))/num_classes)\n",
        "  print(\"----------\")\n",
        "  print()\n",
        "\n",
        "\n",
        "print(\"\\n\\nPrecision Score averaged per number of children\")\n",
        "num_classes = 0\n",
        "mean_so_far = 0\n",
        "for num_kids, labels in num_of_kids.items():\n",
        "  print(\"Number_of_kids:\",num_kids)\n",
        "  print(\"Number of classes:\",len(labels))\n",
        "  precs = []\n",
        "  precs_ordered = []\n",
        "  num_classes_so_far = num_classes\n",
        "  num_classes += len(labels)\n",
        "  for label_idx in labels:\n",
        "    if label_idx in labels_map_index:\n",
        "      precs.append(precision[labels_map_index[label_idx]-1])\n",
        "      precs_ordered.append((precision[labels_map_index[label_idx]-1],all_labels[label_idx]))\n",
        "\n",
        "  precs_ordered = sorted(precs_ordered, reverse=False, key=lambda x: x[0])\n",
        "  maix = min(5,len(precs_ordered))\n",
        "  print(precs_ordered[:maix])\n",
        "\n",
        "  mean_so_far = (mean_so_far*num_classes_so_far+np.mean(precs)*len(labels))/num_classes\n",
        "  \n",
        "  print(\"precision:\",np.mean(precs))\n",
        "  print(\"Avg precision until here:\",(mean_so_far*num_classes_so_far+np.mean(precs)*len(labels))/num_classes)\n",
        "  print(\"----------\")\n",
        "  print()\n",
        "\n",
        "F1s = 2 * (precision * recall)/(precision + recall)\n",
        "F1s = np.nan_to_num(F1s)\n",
        "#print(F1s)\n",
        "\n",
        "\n",
        "print(\"\\n\\nF1 Score averaged per number of children\")\n",
        "num_classes = 0\n",
        "mean_so_far = 0\n",
        "for num_kids, labels in num_of_kids.items():\n",
        "  print(\"Number_of_kids:\",num_kids)\n",
        "  print(\"Number of classes:\",len(labels))\n",
        "  F1s_list = []\n",
        "  F1s_list_label = []\n",
        "  num_classes_so_far = num_classes\n",
        "  num_classes += len(labels)\n",
        "  for label_idx in labels:\n",
        "    if label_idx in labels_map_index:\n",
        "      F1s_list.append(F1s[labels_map_index[label_idx]-1])\n",
        "      F1s_list_label.append((F1s[labels_map_index[label_idx]-1], all_labels[label_idx]))\n",
        "  \n",
        "  F1s_list_ordered = sorted(F1s_list_label, reverse=False, key=lambda x: x[0])\n",
        "  maix = min(5,len(F1s_list))\n",
        "  print(F1s_list_ordered[:maix])\n",
        "\n",
        "\n",
        "  mean_so_far = (mean_so_far*num_classes_so_far+np.mean(F1s_list)*len(labels))/num_classes\n",
        "  \n",
        "  \n",
        "  print(\"F1:\",np.mean(F1s_list))\n",
        "  print(\"Avg F1 until here:\",(mean_so_far*num_classes_so_far+np.mean(F1s_list)*len(labels))/num_classes)\n",
        "  print(\"----------\")\n",
        "  print()\n",
        "\n",
        "weighted_F1s = weights * F1s\n",
        "\n",
        "macro_f1 = np.mean(F1s)\n",
        "print(macro_f1)\n",
        "\n",
        "weighted_f1 = np.sum(weighted_F1s)\n",
        "print(weighted_f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3qYT8n946pe",
        "colab_type": "text"
      },
      "source": [
        "# Clean Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw4nV2054_M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define checkpoint_path and predict rdf:type of images\n",
        "checkpoint_path = model_dir+\"/model.ckpt-9000\"\n",
        "print(\"predict with checkpoint:\",checkpoint_path)\n",
        "predictions = est.predict(input_fn=input_fn_builder_unshuffled(is_training=False),yield_single_examples=True, checkpoint_path=checkpoint_path)\n",
        "prediction_output = list(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiQDNAxc5cMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return filename of images that are predicted\n",
        "data = input_fn_builder_unshuffled(is_training=False, only_filenames=True)(params)\n",
        "labels_and_filenames = []\n",
        "\n",
        "for elem in data:\n",
        "  labels_and_filenames.append(elem)\n",
        "\n",
        "file_names = [x[1].numpy()[0].decode(\"utf-8\") for x in labels_and_filenames]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFjVTVXCCycz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Zip file names and predictions\n",
        "zipped_data = list(zip(file_names,prediction_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jemruyb57C8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the difference between the highest confidence and the ground truth confidence as a rejection criterion\n",
        "difference_max_conf_to_gt_conf = 0.3\n",
        "\n",
        "# Reduce dataset by images that are probably mislabeled\n",
        "remaining_files = []\n",
        "count = 0\n",
        "for x in zipped_data:\n",
        "  file_name = x[0]\n",
        "  label = x[2][\"label\"]-1\n",
        "  prediction = x[2][\"prediction\"]\n",
        "  if label != prediction:\n",
        "    max_conf = np.max(x[2][\"probabilities\"])\n",
        "    gt_conf = x[2][\"probabilities\"][label]\n",
        "    prediction_is_likely = True if max_conf-gt_conf > 0.3 else False\n",
        "    if prediction_is_likely:\n",
        "      count += 1\n",
        "    else:\n",
        "      remaining_files.append((\"gs://ise-bucket/efficientnet/dataset-20-01-13_1-img_10000-ent_100-class/\"+file_name,inverse_label_map[label]))\n",
        "  else:\n",
        "    remaining_files.append((\"gs://ise-bucket/efficientnet/dataset-20-01-13_1-img_10000-ent_100-class/\"+file_name,inverse_label_map[label]))\n",
        "\n",
        "print(count)\n",
        "print(len(remaining_files))\n",
        "print(count/len(zipped_data))\n",
        "df_remaining_files = pd.DataFrame(remaining_files,columns=[\"url\",\"label\"])\n",
        "df_remaining_files.to_csv(\"filtered_training_path_label.csv\")\n",
        "print(df_remaining_files.head(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OPARvgGXd1h",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA7iHiYjMEj0",
        "colab_type": "text"
      },
      "source": [
        "##functions to get predictions and ground truth labels for images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7g0VOGLasR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_with_ckpt_for_ckpt_number(_model_name, _run_name, _ckpt_num):\n",
        "  _model_dir = 'gs://ise-bucket/efficientnet/'+_model_name+'/'+_run_name\n",
        "  checkpoint_path = _model_dir+\"/model.ckpt-\"+_ckpt_num\n",
        "  print(\"predict with checkpoint:\",checkpoint_path)\n",
        "\n",
        "  params = dict(\n",
        "      steps_per_epoch=num_train_images / train_batch_size,\n",
        "      use_bfloat16=False);\n",
        "\n",
        "  save_checkpoints_steps = params[\"steps_per_epoch\"];\n",
        "\n",
        "  config = tf.estimator.tpu.RunConfig(\n",
        "        cluster=tpu_cluster_resolver,\n",
        "        model_dir=_model_dir, \n",
        "        save_checkpoints_steps=save_checkpoints_steps,\n",
        "        keep_checkpoint_max = 100,\n",
        "        log_step_count_steps=log_step_count_steps,\n",
        "        session_config=tf.ConfigProto(\n",
        "            graph_options=tf.GraphOptions(\n",
        "                rewrite_options=rewriter_config_pb2.RewriterConfig(\n",
        "                    disable_meta_optimizer=True))),\n",
        "        tpu_config=tf.estimator.tpu.TPUConfig(\n",
        "            iterations_per_loop=iterations_per_loop,\n",
        "            per_host_input_for_training=tf.estimator.tpu.InputPipelineConfig\n",
        "            .PER_HOST_V2));\n",
        "\n",
        "  est = tf.estimator.tpu.TPUEstimator(\n",
        "        use_tpu=use_tpu,\n",
        "        model_fn=model_fn,\n",
        "        config=config,\n",
        "        train_batch_size=train_batch_size,\n",
        "        eval_batch_size=eval_batch_size,\n",
        "        predict_batch_size = predict_batch_size,\n",
        "        params=params);\n",
        "\n",
        "\n",
        "  predictions = est.predict(input_fn=input_fn_builder_unshuffled(is_training=False),yield_single_examples=True, checkpoint_path=checkpoint_path)\n",
        "  prediction_output = list(predictions)\n",
        "  return prediction_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dXXFiYgYszy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_filenames_for_images():\n",
        "  params = {\"batch_size\":1}\n",
        "  data = input_fn_builder_unshuffled(is_training=False, only_filenames=True)(params)\n",
        "\n",
        "  filenames = []\n",
        "  idx = 0\n",
        "  for elem in data:\n",
        "    idx += 1\n",
        "    if idx % 10000 == 0:\n",
        "      print(idx)\n",
        "    filename = elem[1].numpy()[0].decode(\"utf-8\")\n",
        "    filenames.append(filename)\n",
        "  return filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPESM2rwXjRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_labels_for_images():\n",
        "  fs = gcsfs.GCSFileSystem()\n",
        "  fs.invalidate_cache('gs://ise-bucket/efficientnet/configs/config_9/labels.csv')  \n",
        "  if tf.gfile.Exists('gs://ise-bucket/efficientnet/configs/config_9/labels.csv'):\n",
        "    print(\"load labels.csv\")\n",
        "    labels_df = pd.read_csv('gs://ise-bucket/efficientnet/configs/config_9/labels.csv',header=None)\n",
        "    labels = labels_df[0].values.tolist()\n",
        "  else:\n",
        "    store_labels_for_images()\n",
        "    labels = get_labels_for_images()\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oDcl8xBjpOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_labels_for_images():\n",
        "  print(\"store labels.csv\")\n",
        "  df_labels = pd.read_csv(\"gs://ise-bucket/efficientnet/configs/config_9/validation_path_label.csv\")\n",
        "  filenames_csv = df_labels[\"Input\"].values.tolist()\n",
        "  labels_csv = df_labels[\"Output\"].values.tolist()\n",
        "  filename_to_label_dict = {x[0].split(\"/\")[-1]:x[1] for x in list(zip(filenames_csv,labels_csv))}\n",
        "\n",
        "  filenames = get_filenames_for_images()\n",
        "\n",
        "  labels = [filename_to_label_dict[filename] for filename in filenames]\n",
        "\n",
        "  labels_df = pd.DataFrame(labels,columns=[\"label\"])\n",
        "  labels_df.to_csv(\"gs://ise-bucket/efficientnet/configs/config_9/labels.csv\", index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D0OSHyoMURB",
        "colab_type": "text"
      },
      "source": [
        "##functions to store predictions and ground truth labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9vEAgINMbBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_predictions_to_csv(prediction_output):\n",
        "  list_of_probs = []\n",
        "  list_of_labels = []\n",
        "\n",
        "  for image in prediction_output:\n",
        "    list_of_probs.append(image[\"probabilities\"])\n",
        "    list_of_labels.append(image[\"label\"])\n",
        "\n",
        "  pd.DataFrame(list_of_probs).to_csv(\"confidences.csv\")\n",
        "  pd.DataFrame(list_of_labels).to_csv(\"labels.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmOp4eBZK7qz",
        "colab_type": "text"
      },
      "source": [
        "## create datastructures for metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In6hdTlj7WQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('gs://ise-bucket/efficientnet/hierarchy_lists/dbo classes hierarchies.csv',index_col=None)\n",
        "hierarchies_raw = df.values.tolist()\n",
        "hierarchies = [[i for i in line if type(i) != float] for line in hierarchies_raw]\n",
        "hierarchies = [[i for i in line if i != \"owl:Thing\"] for line in hierarchies]\n",
        "hierarchy_list = hierarchies[:-10]\n",
        "\n",
        "\n",
        "all_labels = []\n",
        "all_labels_index = []\n",
        "all_labels_to_index = {}\n",
        "for idx, hierarchy in enumerate(hierarchy_list):\n",
        "  leaf_node_of_hierarchy = hierarchy[-1]\n",
        "  all_labels.append(leaf_node_of_hierarchy)\n",
        "  all_labels_index.append(idx)\n",
        "  all_labels_to_index[leaf_node_of_hierarchy] = idx\n",
        "\n",
        "print(all_labels)\n",
        "print(all_labels_index)\n",
        "print(all_labels_to_index)\n",
        "print()\n",
        "\n",
        "\n",
        "df = pd.read_csv('gs://ise-bucket/efficientnet/configs/config_3/labels_map.csv',index_col=None)\n",
        "labels_map = dict(zip(df.label, df.label_index))\n",
        "labels_map_reverse = dict(zip(df.label_index, df.label))\n",
        "labels_map_index = {all_labels_to_index[k]:v for k,v in labels_map.items()}\n",
        "labels_map_index_reverse = {v:all_labels_to_index[k] for k,v in labels_map.items()}\n",
        "print(labels_map)\n",
        "print(labels_map_reverse)\n",
        "print(labels_map_index)\n",
        "print(labels_map_index_reverse)\n",
        "print()\n",
        "\n",
        "\n",
        "labels_with_predictions = list(labels_map.keys())\n",
        "labels_with_predictions_index = [all_labels_to_index[label] for label in labels_with_predictions]\n",
        "print(labels_with_predictions)\n",
        "print(labels_with_predictions_index)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "hierarchy_dict = {}\n",
        "hierarchy_dict_index = {}\n",
        "for idx, hierarchy in enumerate(hierarchy_list):\n",
        "  leaf_node_of_hierarchy = hierarchy[-1]\n",
        "  hierarchy_dict[leaf_node_of_hierarchy] = hierarchy\n",
        "\n",
        "  leaf_node_idx = all_labels_to_index[leaf_node_of_hierarchy]\n",
        "  hierarchy_idxs = [all_labels_to_index[elem] for elem in hierarchy]\n",
        "  hierarchy_dict_index[leaf_node_idx] = hierarchy_idxs\n",
        "\n",
        "print(hierarchy_dict)\n",
        "print(hierarchy_dict_index)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "labels_per_level = {1:[],2:[],3:[],4:[],5:[],6:[],7:[]}\n",
        "labels_per_level_index = {1:[],2:[],3:[],4:[],5:[],6:[],7:[]}\n",
        "for idx, hierarchy in enumerate(hierarchy_list):\n",
        "  level = len(hierarchy)\n",
        "  leaf_node_of_hierarchy = hierarchy[-1]\n",
        "  labels_per_level[level].append(leaf_node_of_hierarchy)\n",
        "  leaf_node_idx = all_labels_to_index[leaf_node_of_hierarchy]\n",
        "  labels_per_level_index[level].append(leaf_node_idx)\n",
        "\n",
        "print(labels_per_level)\n",
        "print(labels_per_level_index)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "child_parent_dict = {}\n",
        "child_parent_dict_index = {}\n",
        "for idx, hierarchy in enumerate(hierarchy_list):\n",
        "  if len(hierarchy)>1:\n",
        "    child = hierarchy[-1]\n",
        "    child_idx = all_labels_to_index[child]\n",
        "    parent = hierarchy[-2]\n",
        "    parent_idx = all_labels_to_index[parent]\n",
        "    child_parent_dict[child] = parent\n",
        "    child_parent_dict_index[child_idx] = parent_idx\n",
        "\n",
        "print(child_parent_dict)\n",
        "print(child_parent_dict_index)\n",
        "print()\n",
        "\n",
        "inverse_label_map = {value-1:key for key,value in labels_map.items()}\n",
        "\n",
        "lvl1_label_to_index = {label:index for index,label in enumerate(labels_per_level[1])}\n",
        "print(lvl1_label_to_index)\n",
        "print()\n",
        "\n",
        "level_of_label_index = {v[-1]:len(v) for k,v in hierarchy_dict_index.items()}\n",
        "print(level_of_label_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0j96nfKM1u5",
        "colab_type": "text"
      },
      "source": [
        "## functions to calculate metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_mFIHlD05gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate aggregated confidences basend on the class hierarchy\n",
        "def calc_agg_confs(_prediction_output):\n",
        "  agg_conf_per_image_dict = {}\n",
        "  top_pred_per_level_per_image = {}\n",
        "\n",
        "  levels_desc = list(labels_per_level.keys())\n",
        "  levels_desc.sort(reverse=True)\n",
        "\n",
        "  for ent_index,ent in enumerate(_prediction_output): #for each image\n",
        "    if ent_index % 10000 == 0:\n",
        "      print(ent_index)\n",
        "\n",
        "    top_pred_per_level = {}\n",
        "\n",
        "    label_conf = {label_idx:0 for label_idx in all_labels_index} # initialize confidences for each label with 0\n",
        "\n",
        "    for level in levels_desc: #for each level (in descending order 7 -> 1)\n",
        "      label_idxs = labels_per_level_index[level]\n",
        "\n",
        "      level_top_pred_conf = 0\n",
        "      level_top_pred_idx = 0\n",
        "\n",
        "      for label_idx in label_idxs:\n",
        "          if label_idx in labels_with_predictions_index: #update own value\n",
        "            index = labels_map_index[label_idx]-1\n",
        "            label_conf[label_idx] += ent[\"probabilities\"][index]\n",
        "            \n",
        "          if label_idx in child_parent_dict_index.keys(): #update parent value\n",
        "            parent_label_idx = child_parent_dict_index[label_idx]\n",
        "            label_conf[parent_label_idx] += label_conf[label_idx]\n",
        "\n",
        "          if label_conf[label_idx] > level_top_pred_conf: #if it is highest prediction, then update\n",
        "            level_top_pred_conf = label_conf[label_idx]\n",
        "            level_top_pred_idx = label_idx\n",
        "      \n",
        "      top_pred_per_level[level] = (level_top_pred_idx,level_top_pred_conf)\n",
        "\n",
        "    agg_conf_per_image_dict[ent_index] = label_conf\n",
        "    top_pred_per_level_per_image[ent_index] = top_pred_per_level\n",
        "  \n",
        "  return agg_conf_per_image_dict,top_pred_per_level_per_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaXwG7Q-eo6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict with lowest possible level (most finegrained) that is above a certain confidence threshold\n",
        "def calc_acc_lowest_level_above_threshold(_prediction_output, _top_pred_per_level_per_image, labels):\n",
        "  accs_lvl_all = []\n",
        "  rel_data_lvl_all = []\n",
        "  avg_lvl_all = []\n",
        "\n",
        "  levels_desc = list(labels_per_level.keys())\n",
        "  levels_desc.sort(reverse=True)\n",
        "\n",
        "  for conf_threshold in range(0,100,5):  \n",
        "      conf_threshold = conf_threshold/100\n",
        "      scores_lvl_all = 0\n",
        "      tries = 0\n",
        "      levels_of_pred = []\n",
        "\n",
        "      for ent_index,ent in enumerate(_prediction_output):\n",
        "          gt_label = labels[ent_index]\n",
        "          gt_idx = all_labels_to_index[gt_label]\n",
        "          #gt_idx = labels_map_index_reverse[ent[\"label\"]]\n",
        "          gt_hierarchy_idxs = hierarchy_dict_index[gt_idx]\n",
        "          \n",
        "          for level in levels_desc:\n",
        "              top_pred = _top_pred_per_level_per_image[ent_index][level]\n",
        "              top_pred_conf = top_pred[1]\n",
        "\n",
        "              if top_pred_conf >= conf_threshold:\n",
        "                  tries += 1\n",
        "                  top_pred_idx = top_pred[0]\n",
        "\n",
        "                  if top_pred_idx in gt_hierarchy_idxs:\n",
        "                      scores_lvl_all += 1\n",
        "                      levels_of_pred.append(level)\n",
        "                  break;\n",
        "\n",
        "      acc = scores_lvl_all/tries if tries > 0 else 0\n",
        "      #print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, acc, tries/len(_prediction_output), np.mean(levels_of_pred)))\n",
        "      accs_lvl_all.append(acc)\n",
        "      rel_data_lvl_all.append(tries/len(_prediction_output))\n",
        "      avg_lvl_all.append(np.mean(levels_of_pred))\n",
        "  return accs_lvl_all,rel_data_lvl_all,avg_lvl_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCTesjKLC8qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict always with one level above original prediction \n",
        "def calc_acc_one_level_up(_prediction_output, _agg_conf_per_image_dict, labels):\n",
        "  accs_lvl1 = []\n",
        "  rel_data_lvl1 = []\n",
        "  avg_lvl1 = []\n",
        "\n",
        "  for conf_threshold in range(0,100,5):  \n",
        "      conf_threshold = conf_threshold/100\n",
        "      scores_lvl1 = 0\n",
        "      tries = 0\n",
        "      levels = []\n",
        "\n",
        "      for ent_index,ent in enumerate(_prediction_output):\n",
        "          gt_label = labels[ent_index]\n",
        "          gt_idx = all_labels_to_index[gt_label]\n",
        "          gt_hierarchy_idxs = hierarchy_dict_index[gt_idx]\n",
        "\n",
        "          pred_idx = labels_map_index_reverse[ent[\"prediction\"]+1]\n",
        "          pred_hierarchy_idxs = hierarchy_dict_index[pred_idx]\n",
        "\n",
        "          if pred_idx in child_parent_dict_index.keys(): \n",
        "            pred_idx = child_parent_dict_index[pred_idx]\n",
        "          \n",
        "          pred_conf = agg_conf_per_image_dict[ent_index][pred_idx]\n",
        "\n",
        "          if pred_conf >= conf_threshold:\n",
        "              tries += 1\n",
        "              if pred_idx in gt_hierarchy_idxs:\n",
        "                  scores_lvl1 += 1\n",
        "                  levels.append(level_of_label_index[pred_idx])\n",
        "                  \n",
        "      acc = scores_lvl1/tries if tries > 0 else 0\n",
        "      #print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, acc, tries/len(_prediction_output), np.mean(levels)))\n",
        "      accs_lvl1.append(acc)\n",
        "      rel_data_lvl1.append(tries/len(_prediction_output))\n",
        "      avg_lvl1.append(np.mean(levels))\n",
        "  return accs_lvl1, rel_data_lvl1, avg_lvl1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUDkj4W9FMCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict with lowest possible level of the hierarchy of the original prediction\n",
        "def calc_acc_one_level_up_if_below_threshold(_prediction_output, _agg_conf_per_image_dict, labels):\n",
        "  accs_lvl1 = []\n",
        "  rel_data_lvl1 = []\n",
        "  avg_lvl1 = []\n",
        "\n",
        "  for conf_threshold in range(0,100,5):  \n",
        "      conf_threshold = conf_threshold/100\n",
        "      scores_lvl1 = 0\n",
        "      tries = 0\n",
        "      levels = []\n",
        "\n",
        "      for ent_index,ent in enumerate(_prediction_output):\n",
        "          gt_label = labels[ent_index]\n",
        "          gt_idx = all_labels_to_index[gt_label]\n",
        "          gt_hierarchy_idxs = hierarchy_dict_index[gt_idx]\n",
        "\n",
        "          pred_idx = labels_map_index_reverse[ent[\"prediction\"]+1]\n",
        "          pred_hierarchy_idxs = hierarchy_dict_index[pred_idx]\n",
        "\n",
        "          pred_conf = agg_conf_per_image_dict[ent_index][pred_idx]\n",
        "\n",
        "          while pred_conf < conf_threshold:\n",
        "            if pred_idx in child_parent_dict_index.keys(): \n",
        "              pred_idx = child_parent_dict_index[pred_idx]\n",
        "              pred_conf = agg_conf_per_image_dict[ent_index][pred_idx]\n",
        "            else:\n",
        "              break\n",
        "\n",
        "          if pred_conf >= conf_threshold:\n",
        "              tries += 1\n",
        "              if pred_idx in gt_hierarchy_idxs:\n",
        "                  scores_lvl1 += 1\n",
        "                  levels.append(level_of_label_index[pred_idx])\n",
        "                        \n",
        "      acc = scores_lvl1/tries if tries > 0 else 0\n",
        "      #print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, acc, tries/len(_prediction_output), np.mean(levels)))\n",
        "      accs_lvl1.append(acc)\n",
        "      rel_data_lvl1.append(tries/len(_prediction_output))\n",
        "      avg_lvl1.append(np.mean(levels))\n",
        "  return accs_lvl1, rel_data_lvl1, avg_lvl1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VrT4QsCH04X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict always on top level\n",
        "def calc_acc_top_level(_prediction_output, _top_pred_per_level_per_image, labels):\n",
        "  accs_lvl1 = []\n",
        "  rel_data_lvl1 = []\n",
        "  avg_lvl1 = []\n",
        "\n",
        "  for conf_threshold in range(0,100,5):  \n",
        "      conf_threshold = conf_threshold/100\n",
        "      scores_lvl1 = 0\n",
        "      tries = 0\n",
        "      levels = []\n",
        "\n",
        "      for ent_index,ent in enumerate(_prediction_output):\n",
        "          gt_label = labels[ent_index]\n",
        "          gt_idx = all_labels_to_index[gt_label]\n",
        "          gt_hierarchy_idxs = hierarchy_dict_index[gt_idx]\n",
        "          gt_hierarchy_top_label_idx = gt_hierarchy_idxs[0]\n",
        "\n",
        "          pred_lvl1 = _top_pred_per_level_per_image[ent_index][1]\n",
        "          pred_idx = pred_lvl1[0]\n",
        "          pred_conf = pred_lvl1[1]\n",
        "\n",
        "          if pred_conf >= conf_threshold:\n",
        "              tries += 1\n",
        "              if pred_idx == gt_hierarchy_top_label_idx:\n",
        "                  scores_lvl1 += 1\n",
        "                  levels.append(1)\n",
        "\n",
        "      acc = scores_lvl1/tries if tries > 0 else 0\n",
        "      #print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, acc, tries/len(_prediction_output), np.mean(levels)))\n",
        "      accs_lvl1.append(acc)\n",
        "      rel_data_lvl1.append(tries/len(_prediction_output))\n",
        "      avg_lvl1.append(np.mean(levels))\n",
        "  return accs_lvl1, rel_data_lvl1, avg_lvl1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc1-5NhLJWR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_acc_baseline(_prediction_output, labels):\n",
        "  accs = []\n",
        "  rel_data = []\n",
        "  avg_lvl = []\n",
        "  for conf_threshold in range(0,100,5):  \n",
        "    conf_threshold = conf_threshold/100\n",
        "    scores = 0\n",
        "    tries = 0\n",
        "    levels_of_pred = []\n",
        "\n",
        "    for ent_index,ent in enumerate(_prediction_output):\n",
        "      gt_label = labels[ent_index]\n",
        "      gt_idx = all_labels_to_index[gt_label]\n",
        "      \n",
        "      pred_idx = labels_map_index_reverse[ent[\"prediction\"]+1]\n",
        "      pred_conf = ent[\"probabilities\"][ent[\"prediction\"]]\n",
        "\n",
        "      if pred_conf >= conf_threshold:\n",
        "        tries += 1\n",
        "        if gt_idx == pred_idx:\n",
        "          scores += 1\n",
        "          levels_of_pred.append(level_of_label_index[pred_idx])\n",
        "\n",
        "    acc = scores/tries if tries > 0 else 0\n",
        "    #print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, acc, tries/len(_prediction_output), np.mean(levels_of_pred)))\n",
        "    accs.append(acc)\n",
        "    avg_lvl.append(np.mean(levels_of_pred))\n",
        "    rel_data.append(tries/len(_prediction_output))\n",
        "  return accs, rel_data, avg_lvl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8UF-8--NBcu",
        "colab_type": "text"
      },
      "source": [
        "## functions to store and load metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmE2dsJ7S8hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_eval_metrics(_model_name, _run_name, _ckpt_num, accs, accs_name, rel_data, rel_data_name, avg_lvl, avg_lvl_name):\n",
        "  _model_dir = 'gs://ise-bucket/efficientnet/'+_model_name+'/'+_run_name\n",
        "  pd.DataFrame(accs).to_csv(_model_dir+\"/\"+accs_name+\".csv\", index=False, header=False)\n",
        "  pd.DataFrame(rel_data).to_csv(_model_dir+\"/\"+rel_data_name+\".csv\", index=False, header=False)\n",
        "  pd.DataFrame(avg_lvl).to_csv(_model_dir+\"/\"+avg_lvl_name+\".csv\", index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tq4Tq3yNETV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_and_calc_metrics_for_ckpts(ckpts):\n",
        "  accs = []\n",
        "  rel_data = []\n",
        "  avg_lvl = []\n",
        "  for ckpt in ckpts:\n",
        "    _model_name = ckpt[0]\n",
        "    _run_name = ckpt[1]\n",
        "    _ckpt_num = ckpt[2]\n",
        "\n",
        "    _model_dir = 'gs://ise-bucket/efficientnet/'+_model_name+'/'+_run_name\n",
        "    metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "    accs_lvl_all, rel_data_lvl_all, avg_lvl_all = metrics[\"lvl_all\"]\n",
        "    accs.append(accs_lvl_all)\n",
        "    rel_data.append(rel_data_lvl_all)\n",
        "    avg_lvl.append(avg_lvl_all)\n",
        "  return accs,rel_data,avg_lvl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGJJ9DcaNHA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_and_store_metrics_for_ckpt(_model_name, _run_name, _ckpt_num):\n",
        "  print(\"predict and store metrics for ckpt with number\",_ckpt_num)\n",
        "  prediction_output = predict_with_ckpt_for_ckpt_number(_model_name, _run_name, _ckpt_num)\n",
        "  agg_conf_per_image_dict, top_pred_per_level_per_image = calc_agg_confs(prediction_output)\n",
        "  labels = get_labels_for_images()\n",
        "\n",
        "  accs_lvl1, rel_data_lvl1, avg_lvl1 = calc_acc_top_level(prediction_output,top_pred_per_level_per_image,labels)\n",
        "  accs_lvl_all, rel_data_lvl_all, avg_lvl_all = calc_acc_lowest_level_above_threshold(prediction_output,top_pred_per_level_per_image,labels)\n",
        "  accs,rel_data,avg_lvl = calc_acc_baseline(prediction_output,labels)\n",
        "\n",
        "  store_eval_metrics(_model_name, _run_name, _ckpt_num, accs_lvl1, \"accs_lvl1_\"+_ckpt_num, rel_data_lvl1, \"rel_data_lvl1_\"+_ckpt_num, avg_lvl1, \"avg_lvl1_\"+_ckpt_num)\n",
        "  store_eval_metrics(_model_name, _run_name, _ckpt_num, accs_lvl_all, \"accs_lvl_all_\"+_ckpt_num, rel_data_lvl_all, \"rel_data_lvl_all_\"+_ckpt_num, avg_lvl_all, \"avg_lvl_all_\"+_ckpt_num)\n",
        "  store_eval_metrics(_model_name, _run_name, _ckpt_num, accs, \"accs_\"+_ckpt_num, rel_data, \"rel_data_\"+_ckpt_num, avg_lvl, \"avg_lvl_\"+_ckpt_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzh2cuu8NHjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_for_metrics_for_ckpt(_model_name, _run_name, _ckpt_num):\n",
        "  fs = gcsfs.GCSFileSystem()\n",
        "  _model_dir = 'gs://ise-bucket/efficientnet/'+_model_name+'/'+_run_name\n",
        "  fs.invalidate_cache(_model_dir)\n",
        "  print(\"check for metrics in\",_model_dir,\"for ckpt num\",_ckpt_num)\n",
        "  file_path = _model_dir+\"/accs_lvl1_\"+_ckpt_num+\".csv\"\n",
        "  if tf.gfile.Exists(file_path):\n",
        "    file_path = _model_dir+\"/accs_lvl_all_\"+_ckpt_num+\".csv\"\n",
        "    if tf.gfile.Exists(file_path):\n",
        "      file_path = _model_dir+\"/accs_\"+_ckpt_num+\".csv\"\n",
        "      if tf.gfile.Exists(file_path):\n",
        "        file_path = _model_dir+\"/avg_lvl_\"+_ckpt_num+\".csv\"\n",
        "        if tf.gfile.Exists(file_path):\n",
        "          return True\n",
        "\n",
        "  return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl_n-LielfZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_metrics_for_all_ckpts(_model_name, _run_name):\n",
        "  _model_dir = 'gs://ise-bucket/efficientnet/'+_model_name+'/'+_run_name\n",
        "  \n",
        "  fs = gcsfs.GCSFileSystem()\n",
        "  fs.invalidate_cache(_model_dir)\n",
        "\n",
        "  df_eval = pd.read_csv(_model_dir+\"/checkpoint\",header=None)\n",
        "  ckpt_list = [ckpt.split(\": \")[1].replace('\"','') for ckpt in df_eval[0].values[1:]]\n",
        "  for ckpt in ckpt_list:\n",
        "    _ckpt_num = ckpt.split(\"-\")[-1]\n",
        "    print(\"load metrics for ckpt:\",_model_dir+\"/model.ckpt-\"+_ckpt_num)\n",
        "\n",
        "    if check_for_metrics_for_ckpt(_model_name, _run_name, _ckpt_num) == False:\n",
        "      start_timestamp = datetime.datetime.now() # This time will include compilation time\n",
        "      print(\"Start storing metrics at {}\".format(start_timestamp))\n",
        "      predict_and_store_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "      end_timestamp = datetime.datetime.now() # This time will include compilation time\n",
        "      print(\"Finished storing metrics at {}\".format(end_timestamp))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agwSRzQsNH_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num):\n",
        "\n",
        "  _model_dir = 'gs://ise-bucket/efficientnet/'+_model_name+'/'+_run_name\n",
        "\n",
        "  print(\"load metrics for ckpt:\",_model_dir+\"/model.ckpt-\"+_ckpt_num)\n",
        "  if check_for_metrics_for_ckpt(_model_name, _run_name, _ckpt_num) == True:\n",
        "    accs_lvl1 = pd.read_csv(_model_dir+\"/accs_lvl1_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    accs_lvl1 = [x[0] for x in accs_lvl1]\n",
        "    rel_data_lvl1 = pd.read_csv(_model_dir+\"/rel_data_lvl1_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    rel_data_lvl1 = [x[0] for x in rel_data_lvl1]\n",
        "    avg_lvl1 = pd.read_csv(_model_dir+\"/avg_lvl1_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    avg_lvl1 = [x[0] for x in avg_lvl1]\n",
        "\n",
        "    accs_lvl_all = pd.read_csv(_model_dir+\"/accs_lvl_all_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    accs_lvl_all = [x[0] for x in accs_lvl_all]\n",
        "    rel_data_lvl_all = pd.read_csv(_model_dir+\"/rel_data_lvl_all_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    rel_data_lvl_all = [x[0] for x in rel_data_lvl_all]\n",
        "    avg_lvl_all = pd.read_csv(_model_dir+\"/avg_lvl_all_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    avg_lvl_all = [x[0] for x in avg_lvl_all]\n",
        "\n",
        "    accs = pd.read_csv(_model_dir+\"/accs_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    accs = [x[0] for x in accs]\n",
        "    rel_data = pd.read_csv(_model_dir+\"/rel_data_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    rel_data = [x[0] for x in rel_data]\n",
        "    avg_lvl = pd.read_csv(_model_dir+\"/avg_lvl_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "    avg_lvl = [x[0] for x in avg_lvl]\n",
        "\n",
        "    metrics = {\"lvl1\":(accs_lvl1,rel_data_lvl1,avg_lvl1),\"lvl_all\":(accs_lvl_all,rel_data_lvl_all,avg_lvl_all),\"baseline\":(accs,rel_data,avg_lvl)}\n",
        "\n",
        "    return metrics\n",
        "\n",
        "  else:\n",
        "    predict_and_store_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "    metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE_jxy1-zaU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_baseline_metric_for_ckpt(_model_name, _run_name, _ckpt_num):\n",
        "\n",
        "  _model_dir = 'gs://ise-bucket/efficientnet/'+_model_name+'/'+_run_name\n",
        "\n",
        "  print(\"load baseline metric for ckpt:\",_model_dir+\"/model.ckpt-\"+_ckpt_num)\n",
        "  accs = pd.read_csv(_model_dir+\"/accs_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "  accs = [x[0] for x in accs]\n",
        "  rel_data = pd.read_csv(_model_dir+\"/rel_data_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "  rel_data = [x[0] for x in rel_data]\n",
        "  avg_lvl = pd.read_csv(_model_dir+\"/avg_lvl_\"+_ckpt_num+\".csv\",header=None).values.tolist()\n",
        "  avg_lvl = [x[0] for x in avg_lvl]\n",
        "\n",
        "  metrics = {\"baseline\":(accs,rel_data,avg_lvl)}\n",
        "\n",
        "  return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLp9pkNEPh-M",
        "colab_type": "text"
      },
      "source": [
        "##functions to plot metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWENKLbVghsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_metrics_for_ckpt(_model_name, _run_name, _ckpt_num):\n",
        "  metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "  accs_lvl1,rel_data_lvl1,avg_lvl1 = metrics[\"lvl1\"]\n",
        "  accs_lvl_all,rel_data_lvl_all,avg_lvl_all = metrics[\"lvl_all\"]\n",
        "  accs,rel_data,avg_lvl = metrics[\"baseline\"]\n",
        "\n",
        "  print(\"Results for prediction on top level\")\n",
        "  for i, conf_threshold in enumerate(range(0,100,5)):  \n",
        "      conf_threshold = conf_threshold/100\n",
        "      print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, accs_lvl1[i], rel_data_lvl1[i] , avg_lvl1[i]))\n",
        "  print()\n",
        "\n",
        "  print(\"Results for prediction on lowest level above threshold\")\n",
        "  for i, conf_threshold in enumerate(range(0,100,5)):  \n",
        "      conf_threshold = conf_threshold/100\n",
        "      print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, accs_lvl_all[i], rel_data_lvl_all[i] , avg_lvl_all[i]))\n",
        "  print()\n",
        "\n",
        "  print(\"Results for prediction with baseline classifier\")\n",
        "  for i, conf_threshold in enumerate(range(0,100,5)):  \n",
        "      conf_threshold = conf_threshold/100\n",
        "      print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, accs[i], rel_data[i] , avg_lvl[i]))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYSKVLr5ZeIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_metric_for_ckpt(metric ,_model_name, _run_name, _ckpt_num):\n",
        "  metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "  if metric == \"lvl1\":\n",
        "    accs_lvl1,rel_data_lvl1,avg_lvl1 = metrics[\"lvl1\"]\n",
        "    print(\"Results for prediction on top level\")\n",
        "    \"\"\"\n",
        "    for i, conf_threshold in enumerate(range(0,100,5)):  \n",
        "        conf_threshold = conf_threshold/100\n",
        "        print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, accs_lvl1[i], rel_data_lvl1[i] , avg_lvl1[i]))\n",
        "    print()\n",
        "    \"\"\"\n",
        "  elif metric == \"lvl_all\":\n",
        "    accs_lvl_all,rel_data_lvl_all,avg_lvl_all = metrics[\"lvl_all\"]\n",
        "    print(\"Results for prediction on lowest level above threshold\")\n",
        "    print(\"Accs:\",accs_lvl_all)\n",
        "    print(\"Rel_data:\",rel_data_lvl_all)\n",
        "    print(\"Avg. Depth:\",avg_lvl_all)\n",
        "    \"\"\"\n",
        "    for i, conf_threshold in enumerate(range(0,100,5)):  \n",
        "        conf_threshold = conf_threshold/100\n",
        "        print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, accs_lvl_all[i], rel_data_lvl_all[i] , avg_lvl_all[i]))\n",
        "    print()\n",
        "    \"\"\"\n",
        "  elif metric == \"baseline\":\n",
        "    accs,rel_data,avg_lvl = metrics[\"baseline\"]\n",
        "    print(\"Results for prediction with baseline classifier\")\n",
        "    print(\"Accs:\",accs)\n",
        "    print(\"Rel_data:\",rel_data)\n",
        "    print(\"Avg. Depth:\",avg_lvl)\n",
        "    \"\"\"\n",
        "    for i, conf_threshold in enumerate(range(0,100,5)):  \n",
        "        conf_threshold = conf_threshold/100\n",
        "        print(\"threshold: %.2f \\tacc: %.2f \\t%% of data predicted: %.2f \\tavg predicted level: %.2f\" % (conf_threshold, accs[i], rel_data[i] , avg_lvl[i]))\n",
        "    print()\n",
        "    \"\"\"\n",
        "  else:\n",
        "    print(\"metric not correct\")  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYHG7_9xPhmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_all_metrics_for_ckpt(_model_name, _run_name, _ckpt_num):\n",
        "  metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "  accs_lvl1,rel_data_lvl1,avg_lvl1 = metrics[\"lvl1\"]\n",
        "  accs_lvl_all,rel_data_lvl_all,avg_lvl_all = metrics[\"lvl_all\"]\n",
        "  accs,rel_data,avg_lvl = metrics[\"baseline\"]\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(20,10)) \n",
        "  plt.xlabel('% of data predicted')\n",
        "  plt.ylabel('prediction accuracy') \n",
        "  \n",
        "  ax.plot(rel_data,accs,\".-r\", label=\"standard classifier predictions above threshold\")\n",
        "  ax.plot(rel_data_lvl_all,accs_lvl_all,\".-g\",label=\"predictions on lowest possible level above threshold\")\n",
        "  ax.plot(rel_data_lvl1,accs_lvl1,\".-b\",label=\"predictions on top entity level above threshold\")\n",
        "\n",
        "  ax2 = ax.twinx()  \n",
        "  ax2.set_ylabel(\"average level predicted\")\n",
        "\n",
        "  ax2.plot(rel_data,avg_lvl,\":r\")\n",
        "  ax2.plot(rel_data_lvl1,avg_lvl1,\":b\")\n",
        "  ax2.plot(rel_data_lvl_all,avg_lvl_all,\":g\")\n",
        "\n",
        "  ax.set_xlim(1.05, -0.05) \n",
        "  plt.xticks(np.arange(0, 1.1, step=0.1))\n",
        "  extraString = 'confidence threshold'\n",
        "  handles, _ = ax.get_legend_handles_labels()\n",
        "  handles.append(mpatches.Patch(color='none', label=extraString))\n",
        "  plt.legend(handles=handles,loc=\"lower right\")\n",
        "  annotations = [x/100 for x in list(range(0,100,5))]\n",
        "  for index,annotation in enumerate(annotations):\n",
        "    if (index+1) % 2 == 0:\n",
        "      ax.text(rel_data[index]+0.01, accs[index]+0.005,annotation,va=\"bottom\")\n",
        "      ax.text(rel_data_lvl1[index]+0.01, accs_lvl1[index]+0.005,annotation,va=\"bottom\")\n",
        "      ax.text(rel_data_lvl_all[index]+0.01, accs_lvl_all[index]+0.005,annotation,va=\"bottom\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv1RVEM7coCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_all_metrics_for_ckpt2(_model_name, _run_name, _ckpt_num):\n",
        "  metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "  accs_lvl1,rel_data_lvl1,avg_lvl1 = metrics[\"lvl1\"]\n",
        "  accs_lvl_all,rel_data_lvl_all,avg_lvl_all = metrics[\"lvl_all\"]\n",
        "  accs,rel_data,avg_lvl = metrics[\"baseline\"]\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(20,10)) \n",
        "  plt.xlabel('accuracy of prediction multiplied with relative amount of data predicted')\n",
        "  plt.ylabel('average level of prediction') \n",
        "\n",
        "  rel_accs_combined = [x[0]*x[1] for x in zip(rel_data,accs)]\n",
        "  ax.plot(rel_accs_combined,avg_lvl,\"-r\", label=\"standard classifier predictions above threshold\")\n",
        "\n",
        "  rel_accs_combined = [x[0]*x[1] for x in zip(rel_data_lvl1,accs_lvl1)]\n",
        "  ax.plot(rel_accs_combined, avg_lvl1,\"-b\",label=\"predictions on lowest possible level above threshold\")\n",
        "\n",
        "  rel_accs_combined = [x[0]*x[1] for x in zip(rel_data_lvl_all,accs_lvl_all)]\n",
        "  ax.plot(rel_accs_combined, avg_lvl_all, \"-g\",label=\"predictions on top entity level above threshold\")\n",
        "  annotations = [x/100 for x in list(range(0,100,5))]\n",
        "  for index,annotation in enumerate(annotations):\n",
        "    if (index+1) % 2 == 0:\n",
        "      ax.text(rel_accs_combined[index]+0.01,avg_lvl_all[index]+0.005,annotation,va=\"bottom\")\n",
        "\n",
        "  ax.set_xlim(0, 1) \n",
        "  plt.legend(loc=\"upper right\")\n",
        "  #plt.xticks(np.arange(0, 1.1, step=0.1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in6dLJnhCcuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_metrics_for_ckpts(ckpts):\n",
        "  accs,rel_data,avg_lvl = predict_and_calc_metrics_for_ckpts(ckpts)\n",
        "  fig, ax = plt.subplots(figsize=(20,10)) \n",
        "  plt.xlabel('% of data predicted')\n",
        "  plt.ylabel('prediction accuracy')     \n",
        "  annotations = [x/100 for x in list(range(0,100,5))]\n",
        "  ax2 = ax.twinx()\n",
        "  ax2.set_ylabel(\"average level predicted\")\n",
        "  format_codes_ax = [\".-r\",\".-b\",\".-g\"]\n",
        "  format_codes_ax2 = [\":r\",\":b\",\":g\"]\n",
        "  colors = [\"red\",\"blue\",\"green\"]\n",
        "  for ckpt_idx, ckpt in enumerate(ckpts):\n",
        "    ax.plot(rel_data[ckpt_idx],accs[ckpt_idx],format_codes_ax[ckpt_idx], label=ckpt)\n",
        "    ax2.plot(rel_data[ckpt_idx],avg_lvl[ckpt_idx],format_codes_ax2[ckpt_idx])\n",
        "\n",
        "  ax.set_xlim(1.05, -0.05) \n",
        "  plt.xticks(np.arange(0, 1.1, step=0.1))\n",
        "  extraString = 'confidence threshold'\n",
        "  handles, labels = ax.get_legend_handles_labels()\n",
        "  handles.append(mpatches.Patch(color='none', label=extraString))\n",
        "  plt.legend(handles=handles,loc=\"center right\")\n",
        "\n",
        "  annotations = [x/100 for x in list(range(0,100,5))]\n",
        "  for index,annotation in enumerate(annotations):\n",
        "    if (index+1) % 2 == 0:\n",
        "      for ckpt_idx, ckpt in enumerate(ckpts):\n",
        "        ax.text(rel_data[ckpt_idx][index]+0.01, accs[ckpt_idx][index]+0.005,annotation,va=\"bottom\",fontdict={\"color\":colors[ckpt_idx]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flegcRiRzDYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_baseline_for_all_ckpts(_model_name,_run_name):\n",
        "  _model_dir = 'gs://ise-bucket/efficientnet/'+_model_name+'/'+_run_name\n",
        "  fs = gcsfs.GCSFileSystem()\n",
        "  fs.invalidate_cache(_model_dir)\n",
        "\n",
        "  accs = []\n",
        "  ckpt_nums = []\n",
        "\n",
        "  baseline_file_path = _model_dir+\"/baseline.csv\"\n",
        "  \n",
        "  if tf.gfile.Exists(baseline_file_path):\n",
        "    df_baseline = pd.read_csv(baseline_file_path)\n",
        "    accs = df_baseline[\"acc\"].values.tolist()\n",
        "    ckpt_nums = df_baseline[\"ckpt_num\"].values.tolist()\n",
        "  else:\n",
        "    df_eval = pd.read_csv(_model_dir+\"/checkpoint\",header=None)\n",
        "    ckpt_list = [ckpt.split(\": \")[1].replace('\"','') for ckpt in df_eval[0].values[1:]]\n",
        "    for ckpt in ckpt_list:\n",
        "      checkpoint_path = _model_dir+\"/\"+ckpt\n",
        "      ckpt_num = ckpt.split(\"-\")[-1]\n",
        "      ckpt_baseline_metric = load_baseline_metric_for_ckpt(_model_name,_run_name,ckpt_num)[\"baseline\"]\n",
        "      accs.append(ckpt_baseline_metric[0][0])\n",
        "      ckpt_nums.append(int(ckpt_num))\n",
        "\n",
        "    df_baseline = pd.DataFrame({\"acc\":accs, \"ckpt_num\":ckpt_nums})\n",
        "    df_baseline.to_csv(baseline_file_path)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(20,10)) \n",
        "  plt.xlabel('step')\n",
        "  plt.ylabel('prediction accuracy')     \n",
        "\n",
        "  ax.plot(ckpt_nums,accs,\"-r\")\n",
        "  max_acc_line_x = np.arange(np.min(ckpt_nums),np.max(ckpt_nums),1)\n",
        "  max_acc_line_y = np.full(max_acc_line_x.shape, np.max(accs))\n",
        "  ax.set_xlim(0, np.max(ckpt_nums)+100) \n",
        "  idx_max = np.argmax(accs)\n",
        "  text = \"max acc.: \"+str(np.max(accs))+\" - ckpt#: \"+str(ckpt_nums[np.argmax(accs)])\n",
        "  ax.text(np.max(ckpt_nums)/2, np.max(accs) ,text,va=\"bottom\")\n",
        "  ax.plot(max_acc_line_x,max_acc_line_y,\"-b\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YX4aFpBn9DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_hierarchy_metric_for_ckpts(_figure_name, _ckpt_list):\n",
        "  # ckpt list with tuples of (_model_name, _run_name, _ckpt_num, _label)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10,7)) \n",
        "  plt.xlabel('% of data predicted')\n",
        "  plt.ylabel('prediction accuracy')     \n",
        "  annotations = [x/100 for x in list(range(0,100,5))]\n",
        "\n",
        "  format_codes_ax = [\"#262523\",\"#12866F\",\"#2569DE\"]\n",
        "  colors = [\"red\",\"blue\",\"green\"]\n",
        "\n",
        "  for idx, ckpt in enumerate(_ckpt_list):\n",
        "    _model_name = ckpt[0]\n",
        "    _run_name = ckpt[1]\n",
        "    _ckpt_num = ckpt[2]\n",
        "    _label = ckpt[3]\n",
        "    metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "    accs_lvl_all,rel_data_lvl_all,avg_lvl_all = metrics[\"lvl_all\"]\n",
        "    ax.plot(rel_data_lvl_all,accs_lvl_all, format_codes_ax[idx], label=_label)\n",
        "  \n",
        "  ax.set_xlim(1.05, -0.05) \n",
        "  plt.xticks(np.arange(0, 1.1, step=0.1))\n",
        "  plt.legend(loc=\"center right\")\n",
        "  plt.savefig(_figure_name, dpi=600, facecolor='w', edgecolor='w',\n",
        "        orientation='portrait', papertype=None, format=None,\n",
        "        transparent=False, bbox_inches=None, pad_inches=0.1, metadata=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG5jKqD83jPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_avg_level_metric_for_ckpts(_figure_name, _ckpt_list):\n",
        "  # ckpt list with tuples of (_model_name, _run_name, _ckpt_num, _label)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10,7)) \n",
        "  plt.xlabel('% of data predicted')\n",
        "  plt.ylabel('avg. prediction level')     \n",
        "  annotations = [x/100 for x in list(range(0,100,5))]\n",
        "\n",
        "  format_codes_ax = [\"#262523\",\"#12866F\",\"#2569DE\"]\n",
        "  colors = [\"red\",\"blue\",\"green\"]\n",
        "\n",
        "  for idx, ckpt in enumerate(_ckpt_list):\n",
        "    _model_name = ckpt[0]\n",
        "    _run_name = ckpt[1]\n",
        "    _ckpt_num = ckpt[2]\n",
        "    _label = ckpt[3]\n",
        "    metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "    accs_lvl_all,rel_data_lvl_all,avg_lvl_all = metrics[\"lvl_all\"]\n",
        "    ax.plot(rel_data_lvl_all,avg_lvl_all, format_codes_ax[idx], label=_label)\n",
        "  \n",
        "  ax.set_xlim(1.05, -0.05) \n",
        "  plt.xticks(np.arange(0, 1.1, step=0.1))\n",
        "  plt.legend(loc=\"center right\")\n",
        "  plt.savefig(_figure_name, dpi=600, facecolor='w', edgecolor='w',\n",
        "        orientation='portrait', papertype=None, format=None,\n",
        "        transparent=False, bbox_inches=None, pad_inches=0.1, metadata=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDlJy-gS7BAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_baseline_vs_tf_hierarchy_metric_for_ckpt(_figure_name, _model_name, _run_name, _ckpt_num):\n",
        "  metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "\n",
        "  accs_lvl_all,rel_data_lvl_all,avg_lvl_all = metrics[\"lvl_all\"]\n",
        "  accs,rel_data,avg_lvl = metrics[\"baseline\"]\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10,7)) \n",
        "  plt.xlabel('% of data predicted')\n",
        "  plt.ylabel('prediction accuracy') \n",
        "\n",
        "  ax.plot(rel_data,accs,\"#12866F\", label=\"standard classifier predictions above threshold\")\n",
        "  ax.plot(rel_data_lvl_all,accs_lvl_all,\"#262523\",label=\"predictions on lowest possible level above threshold\")\n",
        "\n",
        "  ax.set_xlim(1.05, -0.05) \n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.xticks(np.arange(0, 1.1, step=0.1))\n",
        "\n",
        "  plt.xlabel('% of data predicted')\n",
        "  plt.ylabel('prediction accuracy')     \n",
        "  annotations = [x/100 for x in list(range(0,100,5))]\n",
        "\n",
        "  format_codes_ax = [\"#262523\",\"#12866F\",\"#2569DE\"]\n",
        "  colors = [\"red\",\"blue\",\"green\"]\n",
        "  \n",
        "  plt.savefig(_figure_name, dpi=600, facecolor='w', edgecolor='w',\n",
        "        orientation='portrait', papertype=None, format=None,\n",
        "        transparent=False, bbox_inches=None, pad_inches=0.1, metadata=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8nAXYnu9RzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_baseline_vs_tf_avg_level_metric_for_ckpts(_figure_name, _model_name, _run_name, _ckpt_num):\n",
        "  metrics = load_metrics_for_ckpt(_model_name, _run_name, _ckpt_num)\n",
        "\n",
        "  accs_lvl_all,rel_data_lvl_all,avg_lvl_all = metrics[\"lvl_all\"]\n",
        "  accs,rel_data,avg_lvl = metrics[\"baseline\"]\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10,7)) \n",
        "  \n",
        "  ax.plot(rel_data,avg_lvl,\"#12866F\", label=\"standard classifier predictions above threshold\")\n",
        "  ax.plot(rel_data_lvl_all,avg_lvl_all,\"#262523\",label=\"predictions on lowest possible level above threshold\")\n",
        "\n",
        "  ax.set_xlim(1.05, -0.05) \n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.xticks(np.arange(0, 1.1, step=0.1))\n",
        "\n",
        "  plt.xlabel('% of data predicted')\n",
        "  plt.ylabel('avg. level of prediction')     \n",
        "  annotations = [x/100 for x in list(range(0,100,5))]\n",
        "\n",
        "  format_codes_ax = [\"#262523\",\"#12866F\",\"#2569DE\"]\n",
        "  colors = [\"red\",\"blue\",\"green\"]\n",
        "  \n",
        "  plt.savefig(_figure_name, dpi=600, facecolor='w', edgecolor='w',\n",
        "        orientation='portrait', papertype=None, format=None,\n",
        "        transparent=False, bbox_inches=None, pad_inches=0.1, metadata=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7YJnLuECe6J",
        "colab_type": "text"
      },
      "source": [
        "##Run Evaluation (Create plots for report)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YlBIg2X6kxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_baseline_vs_tf_hierarchy_metric_for_ckpt(\"Baseline vs Hierarchy Acc\",\"efficientnet-b0\",\"FT-28-01-20-8\",\"7800\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDcbMH826r-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_baseline_vs_tf_avg_level_metric_for_ckpts(\"Baseline vs Hierarchy Level\",\"efficientnet-b0\",\"FT-28-01-20-8\",\"7000\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kibdlSIpRh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_hierarchy_metric_for_ckpts(\"Dataset L vs Dataset XL Hierarchy\",[(\"efficientnet-b0\",\"FT-28-01-20-8\",\"7800\", \"Model B0 - Dataset L\"),(\"efficientnet-b0\",\"FT-02-02-20-4\",\"35000\", \"Model B0 - Dataset XL\")])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6fgaCTN3xJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg_level_metric_for_ckpts(\"Dataset L vs Dataset XL Level\",[(\"efficientnet-b0\",\"FT-28-01-20-8\",\"7800\", \"Model B0 - Dataset L\"),(\"efficientnet-b0\",\"FT-02-02-20-4\",\"35000\", \"Model B0 - Dataset XL\")])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3ZqpbhcxBG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_hierarchy_metric_for_ckpts(\"Dataset Quality Hierarchy\",[(\"efficientnet-b0\",\"FT-28-01-20-8\",\"7800\", \"Model B0 - Dataset L\"),(\"efficientnet-b0\",\"FT-30-01-20-2\",\"7000\", \"Model B0 - Dataset L Denoised\"),(\"efficientnet-b0\",\"FT-02-02-20-5\",\"6800\", \"Model B0 - Dataset L Balanced\")])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lXubzbB3_fL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg_level_metric_for_ckpts(\"Dataset Quality Level\",[(\"efficientnet-b0\",\"FT-28-01-20-8\",\"7800\", \"Model B0 - Dataset L\"),(\"efficientnet-b0\",\"FT-30-01-20-2\",\"7000\", \"Model B0 - Dataset L Denoised\"),(\"efficientnet-b0\",\"FT-02-02-20-5\",\"6800\", \"Model B0 - Dataset L Balanced\")])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EoY_il9yR-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_hierarchy_metric_for_ckpts(\"Learning Strategies Hierarchy\",[(\"efficientnet-b0\",\"FT-28-01-20-8\",\"7800\", \"Model B0 - Pretrained Initialization\"),(\"efficientnet-b0\",\"FT-02-02-20-6-scratch\",\"29375\", \"Model B0 - Training from Scratch\"),(\"efficientnet-b0\",\"FT-02-02-20-10-warm\",\"6000\", \"Model B0 - Pretrained Initialization with FC Head Warm Start\")])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7wKm-bY4VuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg_level_metric_for_ckpts(\"Learning Strategies Level\",[(\"efficientnet-b0\",\"FT-28-01-20-8\",\"7800\", \"Model B0 - Pretrained Initialization\"),(\"efficientnet-b0\",\"FT-02-02-20-6-scratch\",\"29375\", \"Model B0 - Training from Scratch\"),(\"efficientnet-b0\",\"FT-02-02-20-10-warm\",\"6000\", \"Model B0 - Pretrained Initialization with FC Head Warm Start\")])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFG1a2OcIPRE",
        "colab_type": "text"
      },
      "source": [
        "#Predict single Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TeLkTj3VHVW",
        "colab_type": "text"
      },
      "source": [
        "functions to predict single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrbALKOnBNp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import utils as ut\n",
        "import preprocessing\n",
        "from IPython import display\n",
        "\n",
        "class EvalCkptDriver(ut.EvalCkptDriver):\n",
        "  \"\"\"A driver for running eval inference.\"\"\"\n",
        "\n",
        "  def build_model(self, features, is_training):\n",
        "    \"\"\"Build model using the model_name given through the command line.\"\"\"\n",
        "    model_builder = efficientnet_builder\n",
        "\n",
        "    \n",
        "    features -= tf.constant(\n",
        "        model_builder.MEAN_RGB, shape=[1, 1, 3], dtype=features.dtype)\n",
        "    features /= tf.constant(\n",
        "        model_builder.STDDEV_RGB, shape=[1, 1, 3], dtype=features.dtype)\n",
        "    \n",
        "    output_layer, endpoints = model_builder.build_model(\n",
        "        features,\n",
        "        model_name=model_name,\n",
        "        fine_tuning = True,\n",
        "        pooled_features_only = False,\n",
        "        features_only = False,\n",
        "        training=is_training,\n",
        "        model_dir=model_dir) #model_dir used for saving configs\n",
        "\n",
        "      \n",
        "    # new architecture\n",
        "\n",
        "    def dense_kernel_initializer(shape, dtype=None, partition_info=None):\n",
        "      del partition_info\n",
        "      init_range = 1.0 / np.sqrt(shape[1])\n",
        "      return tf.random_uniform(shape, -init_range, init_range, dtype=dtype)\n",
        "\n",
        "    _fc = tf.layers.Dense(\n",
        "            num_label_classes,\n",
        "            kernel_initializer=dense_kernel_initializer,name = \"ldense\")\n",
        "      \n",
        "    logits = _fc(endpoints['global_pool'])\n",
        "\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    probs = tf.squeeze(probs)\n",
        "\n",
        "    return probs\n",
        "\n",
        "  def get_preprocess_fn(self):\n",
        "    \"\"\"Build input dataset.\"\"\"\n",
        "    return preprocessing.preprocess_image\n",
        "\n",
        "  def eval_example_images(self,\n",
        "                          ckpt_dir,\n",
        "                          image_files,\n",
        "                          labels_map_reverse,\n",
        "                          enable_ema=True,\n",
        "                          export_ckpt=None):\n",
        "    \"\"\"Eval a list of example images.\n",
        "\n",
        "    Args:\n",
        "      ckpt_dir: str. Checkpoint directory path.\n",
        "      image_files: List[str]. A list of image file paths.\n",
        "      labels_map_file: str. The labels map file path.\n",
        "      enable_ema: enable expotential moving average.\n",
        "      export_ckpt: export ckpt folder.\n",
        "\n",
        "    Returns:\n",
        "      A tuple (pred_idx, and pred_prob), where pred_idx is the top 5 prediction\n",
        "      index and pred_prob is the top 5 prediction probability.\n",
        "    \"\"\"\n",
        "    classes = labels_map_reverse\n",
        "    out_probs, pred_idx, pred_prob = self.run_inference(\n",
        "        ckpt_dir, image_files, [0] * len(image_files), enable_ema, export_ckpt)\n",
        "    for i in range(len(image_files)):\n",
        "      print('predicted classes for image {}: '.format(image_files[i]))\n",
        "      for j, idx in enumerate(pred_idx[i]):\n",
        "        print('  -> top_{} ({:4.2f}%): {}  '.format(j, pred_prob[i][j] * 100,\n",
        "                                                    classes[idx]))\n",
        "    return out_probs, pred_idx, pred_prob\n",
        "\n",
        "  def run_inference(self,\n",
        "                    ckpt_dir,\n",
        "                    image_files,\n",
        "                    labels,\n",
        "                    enable_ema=True,\n",
        "                    export_ckpt=None):\n",
        "    \"\"\"Build and run inference on the target images and labels.\"\"\"\n",
        "    label_offset = 1 if self.include_background_label else 0\n",
        "    with tf.Graph().as_default(), tf.Session() as sess:\n",
        "      images, labels = self.build_dataset(image_files, labels, False)\n",
        "      probs = self.build_model(images, is_training=False)\n",
        "      if isinstance(probs, tuple):\n",
        "        probs = probs[0]\n",
        "\n",
        "      self.restore_model(sess, ckpt_dir, enable_ema, export_ckpt)\n",
        "\n",
        "      prediction_idx = []\n",
        "      prediction_prob = []\n",
        "      for _ in range(len(image_files) // self.batch_size):\n",
        "        out_probs = sess.run(probs)\n",
        "        idx = np.argsort(out_probs)[::-1]\n",
        "        prediction_idx.append(idx[:5] - label_offset)\n",
        "        prediction_prob.append([out_probs[pid] for pid in idx[:5]])\n",
        "\n",
        "      # Return the top 5 predictions (idx and prob) for each image.\n",
        "      return out_probs, prediction_idx, prediction_prob\n",
        "\n",
        "\n",
        "\n",
        "def get_eval_driver(model_name,image_size,num_classes,include_background_label=False):\n",
        "  return EvalCkptDriver(\n",
        "      model_name=model_name,\n",
        "      batch_size=1,\n",
        "      image_size=input_image_size,\n",
        "      num_classes = num_label_classes,\n",
        "      include_background_label=include_background_label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9TqBHEuh0tn",
        "colab_type": "text"
      },
      "source": [
        "predict single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPROjn3W7ugT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Provide url to image to predict its rdf:type\n",
        "!wget https://s.hs-data.com/picmon/e7/3dxW_7034Lh_l.jpg -O image_to_pred.jpeg\n",
        "\n",
        "# Display image\n",
        "image_file = 'image_to_pred.jpeg'\n",
        "display.display(display.Image(image_file))\n",
        "\n",
        "# Read Labels\n",
        "df = pd.read_csv('gs://ise-bucket/efficientnet/configs/config_3/labels_map.csv',index_col=None)\n",
        "labels_map_reverse = dict(zip(df.label_index, df.label))\n",
        "\n",
        "# Set up Evaludation Driver\n",
        "eval_driver = get_eval_driver(\n",
        "      model_name=model_name,\n",
        "      image_size=input_image_size,\n",
        "      num_classes = num_label_classes,\n",
        "      include_background_label=include_background_label)\n",
        "\n",
        "# Predict\n",
        "out_probs, pred_idx, pred_prob = eval_driver.eval_example_images(model_dir, [image_file], labels_map_reverse)\n",
        "print(\"predicted classes for aggregated confidences:\")\n",
        "agg_conf_per_image_dict,top_pred_per_level_per_image = calc_agg_confs([{\"probabilities\":out_probs}])\n",
        "agg_confs_and_labels = [(agg_conf_per_image_dict[0][label],all_labels[label]) for label in all_labels_index]\n",
        "agg_confs_and_labels = sorted(agg_confs_and_labels, reverse=True, key=lambda x: x[0])\n",
        "for j, prediction in enumerate(conf[:5]):\n",
        "        print('  -> top_{} ({:4.2f}%): {}  '.format(j, prediction[0] * 100,\n",
        "                                                    prediction[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}