# -*- coding: utf-8 -*-
"""Check dbo classes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EHaWrTqlkvff5t6q4nMz0jU8czAxALb6

Collect necessary information to create training and inference datasets:
- Create datasets based on scraped information

##Initialization
"""

!pip install SPARQLWrapper

import threading, re, sys, os, time, csv, requests, random, json, tempfile, math, itertools, google.auth, urllib.request
import pandas as pd
import numpy as np
from urllib.request import urlopen 
from six.moves.urllib.request import urlopen
from SPARQLWrapper import SPARQLWrapper, JSON
from lxml import html
from bs4 import BeautifulSoup
from google.colab import drive
from google.cloud import storage
from google.colab import auth
from datetime import datetime
from six import BytesIO
from PIL import Image

sparql = SPARQLWrapper("http://dbpedia.org/sparql") 

#Define locations & mount Google Drive
directory = "drive/My Drive/ISE/dbo Classes/"
drive.mount("drive", force_remount=True)

#Authentication & initialization Google Cloud
auth.authenticate_user()

with open('/content/adc.json', 'r') as f:
  auth_info = json.load(f)
credentials, project = google.auth.default()

client = storage.Client(credentials=credentials, project='ise-project-259623')
bucket = client.get_bucket('ise-bucket')

"""---

If necessary clean *temp* folder
"""

!rm -r temp
!mkdir temp

"""#Create Training Dataset

##Load dbo classes and entities
*Does not need to be executed*

Read dbo classes list
"""

dbo_classes = []
with open(directory + "dbo Ontology Classes List.txt", "r") as f:
  for l in f:
    dbo_classes.append(l.replace("\n",""))

"""Check number of resources in dbo classes via Sparql"""

entity_counts = []

# Sparql request
for dbo_class in dbo_classes:
  query = """PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
  select distinct count(?entity)
  where {
  ?entity rdf:type dbo:""" + dbo_class + "}"
  sparql.setQuery(query)
  sparql.setReturnFormat(JSON)
  results = sparql.query().convert()
  count = results["results"]["bindings"][0]["callret-0"]["value"]
  entity_counts.append((dbo_class,int(count)))
  if len(entity_counts)%100==0:
    print(len(entity_counts))

class_counts = pd.concat([pd.DataFrame([i], columns = ["class","entity_count"]) for i in entity_counts], ignore_index=True)
class_counts = class_counts.sort_values(by="entity_count",ascending = False)
class_counts.to_csv(directory + "dbo_class_entity_counts.tsv",sep="\t",index=False)

"""---

##**Get X random resources** for top 100 classes

Select top 100 classes
"""

class_counts = pd.read_csv(directory + "dbo_class_entity_counts.tsv",sep="\t")

"""Filtering dbo classes...
* containing many resources without images (only <5% images)
* that are too general to infere ("Image","Agent",...)
"""

rejected_classes = ["Image", "Agent","CareerStation", "OrganisationMember","SportsSeason","SportsEvent",
                    "SportsTeamMember","SportsTeamSeason","TimePeriod", "NCAATeamSeason", "FootballLeagueSeason", 
                    "MotorsportSeason", "Engine", "AutomobileEngine", "RadioStation", "PersonFunction",
                    "SoccerManager", "AmericanFootballPlayer"] # Remove unnecessary dbo classes from list

class_filter = []
for c in class_counts["class"]:
  if c in rejected_classes:
    class_filter.append(False)
  else:
    class_filter.append(True)
class_counts = class_counts[class_filter]
top100_dbo_classes = list(class_counts["class"][:100])
top200_dbo_classes = list(class_counts["class"][:200])

top100_dbo_classes

"""Up to 10k resources:"""

def get_random_resources(number, top_classes):
  rand_resources = {}
  for dbo_class in top_classes:
    sparql = SPARQLWrapper("http://dbpedia.org/sparql") 
    query = """PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
      select distinct ?entity
      where {
      ?entity rdf:type dbo:""" + dbo_class + "} ORDER BY RAND() LIMIT " + str(number)
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    results = sparql.query().convert()
    rand_resources[dbo_class] = [str(i["entity"]["value"]) for i in results["results"]["bindings"]]
    if len(rand_resources)%10 == 0:
      print(len(rand_resources))

  with open(directory + "top100_dbo_" + str(number) + "_random_resources.txt", 'w') as file:
      file.write(json.dumps(rand_resources))

"""More than 10k resources"""

def get_more_random_resources(number, top_n_classes, n):
  rand_resources = {}
  for dbo_class in top_n_classes[:n]:
    print(dbo_class)
    sparql = SPARQLWrapper("http://dbpedia.org/sparql") 
    results = set()
    while len(results) < number:
      request_limit = max(0,min(10000,number) - random.randint(0,100))
      query = """PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
        select distinct ?entity
        where {
        ?entity rdf:type dbo:""" + str(dbo_class) + "} ORDER BY RAND() LIMIT " + str(request_limit)
      sparql.setQuery(query)
      sparql.setReturnFormat(JSON)
      results_json = sparql.query().convert()
      results_list = [str(i["entity"]["value"]) for i in results_json["results"]["bindings"]]
      results.update(results_list)
    rand_resources[dbo_class] = random.sample(results,number)
  with open(directory + "top" + str(n) + "_dbo_" + str(number) + "_random_resources.txt", 'w') as file:
      file.write(json.dumps(rand_resources))

"""**Run** random resource collection"""

number_of_resources = 50000
get_more_random_resources(number_of_resources,top100_dbo_classes,20)
#get_random_resources(number_of_resources,top100_dbo_classes) #up to 10k

"""---

## Scraping Functions

###Load 1k/10k/40k random resources for images
"""

def load_resources_file(filename):
  with open(directory + filename) as file:
    resources = json.load(file)
  return resources

"""### Helper & Image transfer functions to Google Cloud Storage"""

def download_image(url,filename,ignore_small = True):
  #_, filename = tempfile.mkstemp(suffix=".jpg")
  try:
    response = urlopen(url)
    image_data = response.read()
    image_data = BytesIO(image_data)
    pil_image = Image.open(image_data)
  except:
    print("Error in 'download image': " + url)
    return False

  #Check image size
  h, w = pil_image.size
  if ignore_small & (min(h,w) < 45):
    return False
  
  pil_image_rgb = pil_image.convert("RGB")
  pil_image_rgb.save('/tmp/'+filename, format="JPEG", quality=60)
  return True

def upload_image(filename,upload_path):
  blob = bucket.blob(upload_path+filename)
  blob.upload_from_filename('/tmp/'+filename)

def read_tsv(file_name, quotechar=None):
  with open(file_name, "r") as f:
    reader = csv.reader(f, delimiter="\t", quotechar=quotechar)
    lines = []
    for line in reader:
      lines.append(line)
    return lines

def get_thumbnail_url(wikimedia_url):
  html_data = urlopen(wikimedia_url)
  bs = BeautifulSoup(html_data, 'html.parser')
  img_lines = bs.find_all('img', {'src':re.compile('.jpg|.jpeg|.JPEG|.JPG')})
  thumb_url = "http:" + img_lines[0]["src"]
  return thumb_url

def get_image_usage(wikimedia_url):
  pageContent = requests.get(wikimedia_url)
  tree = html.fromstring(pageContent.content)
  links_to_image = tree.xpath('//*[@id="mw-imagepage-section-linkstoimage"]/ul')
  img_usage = len(links_to_image[0])
  return img_usage

def send_notification(title,content, link =""):
  payload = {"value1": title,"value2": content,"value3": link}
  headers = {'content-type': 'application/json', 'Accept-Charset': 'UTF-8'}
  url = "https://maker.ifttt.com/trigger/notification/with/key/dI98kz1YHGe26YbsKmxNKi"
  requests.get(url, params=payload)

"""Create csv with input (img URL in bucket) and output (dbo_class) for training"""

def create_csv(prefix,export_path):
  blob_list = [b for b in bucket.list_blobs(prefix=prefix)]

  result_list = []

  for i in blob_list[:]:
    input_split = "gs://ise-bucket/" + i.name
    output_split = i.name.split("/")[-1].split("_")[0]
    result_list.append([input_split,output_split])
  df = pd.DataFrame(result_list,index = None, columns = ["Input","Output"])
  df.to_csv(export_path, index = False)
  return(df)

"""###Threading Wrapper"""

def get_imageurl_threading(dbo_class,resources,result,idx,limit,upload_path,img_per_article):
  url_count = 0
  print(dbo_class)
  for resource in resources[:]:
    
    #Chose scraping approach
    if img_per_article == 1:
      scraping_result = get_one_imageurl(resource)   #Scrape one image per Wikipedia article
    else:
      scraping_result = get_top_imageurl(resource,img_per_article)  #Scrape the top ranked of x images in Wikipedia article
    

    if not scraping_result == False:
      resource_name = re.sub('[\W]+', '', resource.split("/")[-1]) 
      filename = "{}_{}.jpg".format(dbo_class,resource_name)
      
      if(download_image(scraping_result,filename)):
        upload_image(filename,upload_path)
        url_count += 1
    
    if url_count%200 == 1:
      print("{}: {} images scraped from: {}".format(datetime.now(), url_count, dbo_class))
    
    if url_count%400 == 1:
      send_notification("{} images scraped".format(url_count), dbo_class)

    if url_count >= limit:
      break
  
  return True

"""### Get First Image URL"""

def get_one_imageurl(resource):
  entity_name = resource.split("/")[-1]
  wikipedia_url = "https://en.wikipedia.org/wiki/" + entity_name

  #Open Wikipedia URL
  try:
    html_data = urlopen(wikipedia_url)
  except:
    return False

  #Find images
  try:
    bs = BeautifulSoup(html_data, 'html.parser')
    image_soup = bs.find('img', {'src':re.compile('.jpg|.jpeg|.JPEG|.JPG')}) #TODO: Special characters formatting (e.g. NiÃ±a...)
  except:
    return False

  #Check, if images were found
  if image_soup == None:
    return False

  #Remove too small images by wiki-info
  if "height" in image_soup:
    if int(image_soup["height"]) > 45:
      image = image_soup
    else:
      return False
  else:
    image = image_soup

  #Get reasonable standard wiki-thumbnail size of image
  try:
    filename = image['src'].split("/")[8]
    wikimedia_url = "https://en.wikipedia.org/wiki/File:" + filename
    imgurl = get_thumbnail_url(wikimedia_url)
    return imgurl
  except:
    try:
      filename = image['src'].split("/")[7]
      wikimedia_url = "https://en.wikipedia.org/wiki/File:" + filename
      imgurl = get_thumbnail_url(wikimedia_url)
      return imgurl
    except:
      #if thumbnail is not available, use original image 
      imgurl = "http:" + image['src']
      return imgurl

"""### Get Top Image URL (ranking approach)"""

def get_top_imageurl(resource,img_per_article):
  first_imageurls = get_first_imageurls(resource, img_per_article)
  if first_imageurls==False:
    return False
  
  imgurl_list, metadata = first_imageurls
  scores = calculate_tfidf_scores(len(imgurl_list),metadata)
  idx = np.argmax(scores)
  top_imageurl = imgurl_list[idx]
  if idx != 0:
    print("Image was reranked: " + top_imageurl + " " + str(metadata[idx]))
  return top_imageurl

"""Get upper X image URLs"""

def get_first_imageurls(resource,img_per_article):
  entity_name = resource.split("/")[-1]
  wikipedia_url = "https://en.wikipedia.org/wiki/" + entity_name
  imgurl_list = []
  metadata = []

  #Open Wikipedia URL
  try:
    html_data = urlopen(wikipedia_url)
  except:
    return False
  
  #Find all jpg images in Beautiful Soup
  try:
    bs = BeautifulSoup(html_data, 'html.parser')
    images = bs.find_all('img', {'src':re.compile('.jpg|.jpeg|.JPEG|.JPG')}) #TODO: Special characters formatting (e.g. NiÃ±a...)
  except:
    return False

  #Remove small images
  images2 = [] # list of images larger than 45px
  for image in images:
    if "height" in image:
      if int(image["height"]) > 45:
        images2.append(image)
    else:
      images2.append(image)

  if len(images2) == 0:
    return False
  else:
    #get image URLs
    for img_number, image in enumerate(images2):
      if len(imgurl_list) < img_per_article:
        try:
          filename = image['src'].split("/")[8]
          wikimedia_url = "https://en.wikipedia.org/wiki/File:" + filename
          imgurl = get_thumbnail_url(wikimedia_url)
          img_usage = get_image_usage(wikimedia_url)
          imgurl_list.append(imgurl)
          metadata.append([img_number + 1,img_usage])
        except:
          try:
            filename = image['src'].split("/")[7]
            wikimedia_url = "https://en.wikipedia.org/wiki/File:" + filename
            imgurl = get_thumbnail_url(wikimedia_url)
            img_usage = get_image_usage(wikimedia_url)
            imgurl_list.append(imgurl)
            metadata.append([img_number + 1,img_usage])
          except:
            print(image['src'])
            pass
  if len(imgurl_list) == 0:
    return False
  return imgurl_list, metadata

"""Calculate top image via ranking"""

def calculate_tfidf_scores(number_of_images, metadata):
  # Get tf-idf Scores
  # score = (tf)*(idf)
  if number_of_images != 1:
    scores = [(1/(math.sqrt(int(i[0]))))*(math.log(5968914/int(i[1]))) for i in metadata] 
  else:
    scores = [math.log(5968914/int(metadata[0][1]))]
  return scores

"""### Scraping function"""

def run_scraping(random_resources_file,start_class,no_of_classes,limit,img_per_article):
  random_resources = load_resources_file(random_resources_file)
  dataset_name = name_prefix + "_" + str(img_per_article) + "-img_" + str(limit) + "-ent_" + str(no_of_classes) + "-class"
  upload_path = "efficientnet/" + dataset_name + "/" 
  classes_list = list(random_resources.keys())[start_class:start_class + no_of_classes]
  results = [{} for dbo_class in classes_list]
  threads = []

  start = datetime.now()
  print (dataset_name)
  print(start)
  for idx, dbo_class in enumerate(classes_list):
    # start one thread per dbo class.
    process = threading.Thread(target=get_imageurl_threading, args=(dbo_class,random_resources[dbo_class][:],results,idx,limit,upload_path,img_per_article))
    process.start()
    threads.append(process)
  # pause execution on the main thread by 'joining' all of started threads.
  for i, process in enumerate(threads):
    print("{} - {} - {}".format(i,process,datetime.now()))
    process.join()

  end = datetime.now()
  print("{} to scrape {} classes.".format(end-start, no_of_classes))

  #Training csv for Sebastian
  create_csv(upload_path,directory + dataset_name + "_training.csv")

"""##RUN Scraping

Set Parameters
"""

name_prefix = "dataset-20-01-15"  #@param {type:"string"}
start_class =  0#@param {type:"number"}
no_of_classes =  100#@param ["100", "20"] {type:"raw", allow-input: true}
entity_limit = 2000 #@param ["10000", "50000"] {type:"raw", allow-input: true}
random_resources_file = "top100_dbo_10000_random_resources.txt" #@param ["top100_dbo_10000_random_resources.txt","top100_dbo_1000_random_resources.txt", "top20_dbo_50000_random_resources.txt"]
images_per_article =  1           #@param {type:"integer"}

print(name_prefix + "_" + str(images_per_article) + "-img_" + str(entity_limit) + "-ent_" + str(no_of_classes) + "-class")

"""**Run Scraping**"""

run_scraping(random_resources_file,start_class,no_of_classes,entity_limit,images_per_article)

#Manual CSV creation
create_csv("efficientnet/dataset-20-01-15_1-img_2000-ent_100-class/",directory + "dataset-20-01-15_1-img_2000-ent_100-class_training.csv")

"""---

##Check up Scripts

**Check:** How many images are uploaded
"""

upload_path = "efficientnet/dataset-20-01-13_1-img_10000-ent_100-class/"#.format(dataset_name)  #replace if necessary e.g.: "efficientnet/dataset7/"
blob_list = [b for b in bucket.list_blobs(prefix=upload_path)]

classes = []
for i in blob_list[:]:
  line = i.name.split("/")[-1]
  classes.append(line.split("_")[0])

classes = list(set(classes))
image_counts = {}
for c in classes:
  image_counts[c] = 0

for i in blob_list[:]:
  line = i.name.split("/")[-1]
  image_counts[line.split("_")[0]] += 1

print(image_counts)

"""####(deprecated) Serial Scraping"""

start = datetime.now()
i = 0
for dbo_class in rand_resources_1000:
  i += 1
  if i >= 0:
    get_one_image_link_threading_wrapper(dbo_class,rand_resources_1000[dbo_class])
  end = datetime.now()
  print("{} to scrape {} classes.".format(end-start, i))
  
print("{} to scrape {} classes.".format(end-start, i))

"""Combine individual scraped files"""

line_list = []
for filename in os.listdir(directory + "temp")[:]:
  dbo_class = filename.replace(".txt","")
  with open(directory + "temp/{}".format(filename), 'r') as file:
     line_list.append([dbo_class] + eval(file.read()))
dataset_one_imagelink = pd.DataFrame(line_list).rename(columns={0:"dbo_class"})
dataset_one_imagelink.to_csv(directory + "dataset_one_imagelink_100.tsv",index = False, sep="\t")