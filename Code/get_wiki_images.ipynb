{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of get_wiki_images.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H10AZC7-ZxlW",
        "CtidsMh7E2sX",
        "bYrlrWD7NxIC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgNPrqQ9zoy5",
        "colab_type": "text"
      },
      "source": [
        "##Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mthGf5M3wxzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import threading, re, sys, os, time, csv, requests, random, json, tempfile, math, itertools, google.auth, urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from urllib.request import urlopen \n",
        "from six.moves.urllib.request import urlopen\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from lxml import html\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "from datetime import datetime\n",
        "from six import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\") \n",
        "\n",
        "#Define locations & mount Google Drive\n",
        "directory = \"drive/My Drive/ISE/dbo type hierarchy/\"\n",
        "drive.mount(\"drive\", force_remount=True)\n",
        "\n",
        "#Authentication & initialization Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "with open('/content/adc.json', 'r') as f:\n",
        "  auth_info = json.load(f)\n",
        "credentials, project = google.auth.default()\n",
        "\n",
        "client = storage.Client(credentials=credentials, project='ise-project-259623')\n",
        "bucket = client.get_bucket('ise-bucket')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnPiuxTTEclz",
        "colab_type": "text"
      },
      "source": [
        "#Create Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4weJoEMe9jzC",
        "colab_type": "text"
      },
      "source": [
        "###Load 1k/10k/40k random resources for images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxHhbwAzJVhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_resources_file(filename):\n",
        "  with open(directory + filename) as file:\n",
        "    resources = json.load(file)\n",
        "  return resources"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKGdFzQFmA4N",
        "colab_type": "text"
      },
      "source": [
        "### Helper & Image transfer functions to Google Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFMnMbFmSfnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_image(url,filename,ignore_small = True):\n",
        "  #download one image from google storage bucket\n",
        "  try:\n",
        "    response = urlopen(url)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    pil_image = Image.open(image_data)\n",
        "  except:\n",
        "    print(\"Error in 'download image': \" + url)\n",
        "    return False\n",
        "\n",
        "  #Check image size\n",
        "  h, w = pil_image.size\n",
        "  if ignore_small & (min(h,w) < 45):\n",
        "    return False\n",
        "  try:\n",
        "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "    pil_image_rgb.save('/tmp/'+filename, format=\"JPEG\", quality=60)\n",
        "    return True\n",
        "  except:\n",
        "    print(\"Error in 'download image': \" + url)\n",
        "    return False\n",
        "\n",
        "def upload_image(filename,upload_path):\n",
        "  #upload one image to google storage bucket\n",
        "  blob = bucket.blob(upload_path+filename)\n",
        "  blob.upload_from_filename('/tmp/'+filename)\n",
        "\n",
        "def read_tsv(file_name, quotechar=None):\n",
        "  #read tsv file\n",
        "  with open(file_name, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "    lines = []\n",
        "    for line in reader:\n",
        "      lines.append(line)\n",
        "    return lines\n",
        "\n",
        "def get_thumbnail_url(wikimedia_url):\n",
        "  #load url of thumbnail image\n",
        "  html_data = urlopen(wikimedia_url)\n",
        "  bs = BeautifulSoup(html_data, 'html.parser')\n",
        "  img_lines = bs.find_all('img', {'src':re.compile('.jpg|.jpeg|.JPEG|.JPG')})\n",
        "  thumb_url = \"http:\" + img_lines[0][\"src\"]\n",
        "  return thumb_url\n",
        "\n",
        "def get_image_usage(wikimedia_url):\n",
        "  #scrape value of how often image is used on Wikipedia for Tf-Idf Score\n",
        "  pageContent = requests.get(wikimedia_url)\n",
        "  tree = html.fromstring(pageContent.content)\n",
        "  links_to_image = tree.xpath('//*[@id=\"mw-imagepage-section-linkstoimage\"]/ul')\n",
        "  img_usage = len(links_to_image[0])\n",
        "  return img_usage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO6AcUNvNVnt",
        "colab_type": "text"
      },
      "source": [
        "Create csv with input (img URL in bucket) and output (rdf_type) for training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iIC2AGjwylGv",
        "colab": {}
      },
      "source": [
        "def create_csv(prefix,export_path):\n",
        "  blob_list = [b for b in bucket.list_blobs(prefix=prefix)]\n",
        "\n",
        "  result_list = []\n",
        "\n",
        "  for i in blob_list[:]:\n",
        "    input_split = \"gs://ise-bucket/\" + i.name\n",
        "    output_split = i.name.split(\"/\")[-1].split(\"_\")[0]\n",
        "    result_list.append([input_split,output_split])\n",
        "  df = pd.DataFrame(result_list,index = None, columns = [\"Input\",\"Output\"])\n",
        "  df.to_csv(export_path, index = False)\n",
        "  return(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKJTR-4dri4C",
        "colab_type": "text"
      },
      "source": [
        "###Threading Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8fOzZP3TQmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_imageurl_threading(rdf_type,resources,result,idx,limit,upload_path,img_per_article):\n",
        "  url_count = 0\n",
        "  print(rdf_type)\n",
        "  for resource in resources[:]:\n",
        "    \n",
        "    #Chose scraping approach\n",
        "    if img_per_article == 1:\n",
        "      scraping_result = get_one_imageurl(resource)   #Scrape one image per Wikipedia article\n",
        "    else:\n",
        "      scraping_result = get_top_imageurl(resource,img_per_article)  #Scrape the top ranked of x images in Wikipedia article\n",
        "    \n",
        "\n",
        "    if not scraping_result == False:\n",
        "      resource_name = re.sub('[\\W]+', '', resource.split(\"/\")[-1]) \n",
        "      filename = \"{}_{}.jpg\".format(rdf_type,resource_name)\n",
        "      \n",
        "      if(download_image(scraping_result,filename)):\n",
        "        upload_image(filename,upload_path)\n",
        "        url_count += 1\n",
        "    \n",
        "    if url_count%200 == 1:\n",
        "      print(\"{}: {} images scraped from: {}\".format(datetime.now(), url_count, rdf_type))\n",
        "    \n",
        "    if url_count%400 == 1:\n",
        "      send_notification(\"{} images scraped\".format(url_count), rdf_type)\n",
        "\n",
        "    if url_count >= limit:\n",
        "      break\n",
        "  \n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rC50_U-ltA5",
        "colab_type": "text"
      },
      "source": [
        "### Get First Image URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELmz_gfqRMqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_one_imageurl(resource):\n",
        "  entity_name = resource.split(\"/\")[-1]\n",
        "  wikipedia_url = \"https://en.wikipedia.org/wiki/\" + entity_name\n",
        "\n",
        "  #Open Wikipedia URL\n",
        "  try:\n",
        "    html_data = urlopen(wikipedia_url)\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "  #Find images\n",
        "  try:\n",
        "    bs = BeautifulSoup(html_data, 'html.parser')\n",
        "    image_soup = bs.find('img', {'src':re.compile('.jpg|.jpeg|.JPEG|.JPG')})\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "  #Check, if images were found\n",
        "  if image_soup == None:\n",
        "    return False\n",
        "\n",
        "  #Remove too small images by wiki-info\n",
        "  if \"height\" in image_soup:\n",
        "    if int(image_soup[\"height\"]) > 45:\n",
        "      image = image_soup\n",
        "    else:\n",
        "      return False\n",
        "  else:\n",
        "    image = image_soup\n",
        "\n",
        "  #Get reasonable standard wiki-thumbnail size of image\n",
        "  try:\n",
        "    filename = image['src'].split(\"/\")[8]\n",
        "    wikimedia_url = \"https://en.wikipedia.org/wiki/File:\" + filename\n",
        "    imgurl = get_thumbnail_url(wikimedia_url)\n",
        "    return imgurl\n",
        "  except:\n",
        "    try:\n",
        "      filename = image['src'].split(\"/\")[7]\n",
        "      wikimedia_url = \"https://en.wikipedia.org/wiki/File:\" + filename\n",
        "      imgurl = get_thumbnail_url(wikimedia_url)\n",
        "      return imgurl\n",
        "    except:\n",
        "      #if thumbnail is not available, use original image \n",
        "      imgurl = \"http:\" + image['src']\n",
        "      return imgurl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b259gV6IqczU",
        "colab_type": "text"
      },
      "source": [
        "### Get Top Image URL (ranking approach)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba8vj3Uww2eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_imageurl(resource,img_per_article):\n",
        "  first_imageurls = get_first_imageurls(resource, img_per_article)\n",
        "  if first_imageurls==False:\n",
        "    return False\n",
        "  \n",
        "  imgurl_list, metadata = first_imageurls\n",
        "  scores = calculate_tfidf_scores(len(imgurl_list),metadata)\n",
        "  idx = np.argmax(scores)\n",
        "  top_imageurl = imgurl_list[idx]\n",
        "  if idx != 0:\n",
        "    print(\"Image was reranked: \" + top_imageurl + \" \" + str(metadata[idx]))\n",
        "  return top_imageurl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GKL8StNqj99",
        "colab_type": "text"
      },
      "source": [
        "Get upper X image URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpuv1yWewykU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_first_imageurls(resource,img_per_article):\n",
        "  entity_name = resource.split(\"/\")[-1]\n",
        "  wikipedia_url = \"https://en.wikipedia.org/wiki/\" + entity_name\n",
        "  imgurl_list = []\n",
        "  metadata = []\n",
        "\n",
        "  #Open Wikipedia URL\n",
        "  try:\n",
        "    html_data = urlopen(wikipedia_url)\n",
        "  except:\n",
        "    return False\n",
        "  \n",
        "  #Find all jpg images in Beautiful Soup\n",
        "  try:\n",
        "    bs = BeautifulSoup(html_data, 'html.parser')\n",
        "    images = bs.find_all('img', {'src':re.compile('.jpg|.jpeg|.JPEG|.JPG')})\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "  #Remove small images\n",
        "  images2 = [] # list of images larger than 45px\n",
        "  for image in images:\n",
        "    if \"height\" in image:\n",
        "      if int(image[\"height\"]) > 45:\n",
        "        images2.append(image)\n",
        "    else:\n",
        "      images2.append(image)\n",
        "\n",
        "  if len(images2) == 0:\n",
        "    return False\n",
        "  else:\n",
        "    #get image URLs\n",
        "    for img_number, image in enumerate(images2):\n",
        "      if len(imgurl_list) < img_per_article:\n",
        "        try:\n",
        "          filename = image['src'].split(\"/\")[8]\n",
        "          wikimedia_url = \"https://en.wikipedia.org/wiki/File:\" + filename\n",
        "          imgurl = get_thumbnail_url(wikimedia_url)\n",
        "          img_usage = get_image_usage(wikimedia_url)\n",
        "          imgurl_list.append(imgurl)\n",
        "          metadata.append([img_number + 1,img_usage])\n",
        "        except:\n",
        "          try:\n",
        "            filename = image['src'].split(\"/\")[7]\n",
        "            wikimedia_url = \"https://en.wikipedia.org/wiki/File:\" + filename\n",
        "            imgurl = get_thumbnail_url(wikimedia_url)\n",
        "            img_usage = get_image_usage(wikimedia_url)\n",
        "            imgurl_list.append(imgurl)\n",
        "            metadata.append([img_number + 1,img_usage])\n",
        "          except:\n",
        "            print(image['src'])\n",
        "            pass\n",
        "  if len(imgurl_list) == 0:\n",
        "    return False\n",
        "  return imgurl_list, metadata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8vIwRkWqpzY",
        "colab_type": "text"
      },
      "source": [
        "Calculate top image via ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9yi0vidznf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_tfidf_scores(number_of_images, metadata):\n",
        "  # Get tf-idf Scores\n",
        "  # score = (tf)*(idf)\n",
        "  if number_of_images != 1:\n",
        "    scores = [(1/(math.sqrt(int(i[0]))))*(math.log(5968914/int(i[1]))) for i in metadata] \n",
        "  else:\n",
        "    scores = [math.log(5968914/int(metadata[0][1]))]\n",
        "  return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH6KiDtvMb0V",
        "colab_type": "text"
      },
      "source": [
        "### Scraping function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1oJ9Jj8Prmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_scraping(random_resources_file,start_type,no_of_types,limit,img_per_article):\n",
        "  # run all\n",
        "  random_resources = load_resources_file(random_resources_file)\n",
        "  dataset_name = name_prefix + \"_\" + str(img_per_article) + \"-img_\" + str(limit) + \"-ent_\" + str(no_of_types) + \"-type\"\n",
        "  upload_path = \"efficientnet/\" + dataset_name + \"/\" \n",
        "  types_list = list(random_resources.keys())[start_type:start_type + no_of_types]\n",
        "  results = [{} for rdf_type in types_list]\n",
        "  threads = []\n",
        "\n",
        "  start = datetime.now()\n",
        "  print (dataset_name)\n",
        "  print(start)\n",
        "  for idx, rdf_type in enumerate(types_list):\n",
        "    # start one thread per rdf type.\n",
        "    process = threading.Thread(target=get_imageurl_threading, args=(rdf_type,random_resources[rdf_type][:],results,idx,limit,upload_path,img_per_article))\n",
        "    process.start()\n",
        "    threads.append(process)\n",
        "  # pause execution on the main thread by 'joining' all of started threads.\n",
        "  for i, process in enumerate(threads):\n",
        "    print(\"{} - {} - {}\".format(i,process,datetime.now()))\n",
        "    process.join()\n",
        "\n",
        "  end = datetime.now()\n",
        "  print(\"{} to scrape {} types.\".format(end-start, no_of_types))\n",
        "\n",
        "  #Training csv\n",
        "  create_csv(upload_path,directory + dataset_name + \"_training.csv\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWJFnjSOZWPq",
        "colab_type": "text"
      },
      "source": [
        "##RUN Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfl61zc7I6ZI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TunC5tJstfzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name_prefix = \"dataset-20-01-31\"  #@param {type:\"string\"}\n",
        "start_type =  0#@param {type:\"number\"}\n",
        "no_of_types =  20#@param [\"100\", \"20\"] {type:\"raw\", allow-input: true}\n",
        "entity_limit = 10000 #@param [\"10000\", \"50000\"] {type:\"raw\", allow-input: true}\n",
        "random_resources_file = \"top100_dbo_2000_random_resources_specific\" #@param [\"top100_dbo_10000_random_resources.txt\", \"top100_dbo_1000_random_resources.txt\", \"top20_dbo_50000_random_resources.txt\", \"top100_dbo_50000_random_resources.txt\", \"top100_dbo_2000_random_resources_specific\"]\n",
        "images_per_article =  1           #@param {type:\"integer\"}\n",
        "\n",
        "print(name_prefix + \"_\" + str(images_per_article) + \"-img_\" + str(entity_limit) + \"-ent_\" + str(no_of_types) + \"-type\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRJPxfglIAqm",
        "colab_type": "text"
      },
      "source": [
        "**Run Scraping**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6aF703c07m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_scraping(random_resources_file,start_type,no_of_types,entity_limit,images_per_article)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
